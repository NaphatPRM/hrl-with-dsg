Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs/MinigridEmpty_VisualDebug 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs/MinigridEmpty_VisualDebug/18 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug/18 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug/18/initiation_set_plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug/18/value_function_plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug/18/accepted_events 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MinigridEmpty_VisualDebug/18/rejected_events 
Creating classifier of type cnn
Creating rainbow with sigma=0.5
Created model-free option global-option with option_idx=0
[DSGTrainer] Adding new SalientEvent  SE([1 1])
[Seed=18] Device count: 1 Device Name: NVIDIA GeForce GTX 1080 Ti
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 0 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: 40.914020973723396
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 1 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -65.2433419926092
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 2 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -72.29790629033232
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 3 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -77.33636846602894
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 4 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -97.40171428932808
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 5 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -108.78126797470031
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 6 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -98.85978018958122
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 7 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -119.3758557275869
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 8 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -112.66705092717893
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 9 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -127.7816496801097
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 10 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -127.27929453723482
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 11 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -129.7437488087453
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 12 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -120.73861153493635
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 13 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -146.4473278047517
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 14 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -131.91546698873572
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 15 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -113.99038468371145
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 16 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -147.33401599200442
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 17 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -126.853606734192
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 18 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -147.91714328341186
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 19 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -141.17998330201954
Accepted New Salient Event:  SE([6 6])
[DSGTrainer] Adding new SalientEvent  SE([6 6])
Accepted New Salient Event:  SE([6 6])
[DSGTrainer] Adding new SalientEvent  SE([6 6])
Accepted New Salient Event:  SE([6 6])
[DSGTrainer] Adding new SalientEvent  SE([6 6])
Accepted New Salient Event:  SE([6 6])
[DSGTrainer] Adding new SalientEvent  SE([6 6])
Accepted New Salient Event:  SE([4 2])
[DSGTrainer] Adding new SalientEvent  SE([4 2])
Took 8.737197875976562s to update distance table with 20040 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 20 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -139.75255122967064
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 21 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -154.84228399023414
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 22 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -133.70148913213052
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 23 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -160.6504768356681
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 24 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -152.88320855045458
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 25 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -144.17962354415795
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 26 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -153.71717552491464
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 27 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -156.66356308123795
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 28 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -162.23590063676238
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 29 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -153.0191238700063
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 30 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -165.29746742296265
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 31 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -142.94371525733732
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 32 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -136.66479307203554
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 33 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -174.1128757931292
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 34 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -173.42693657721975
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 35 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -136.899649305793
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 36 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -164.7316453208041
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 37 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -166.3353985566646
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 38 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 1.0	IntrinsicReward: -172.89612243510783
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x149060fd4220>
================================================================================
[Expansion] Episode: 39 Step: 0
================================================================================
Attempting to expand SE([1 1])
[RND Rollout] Reward: 0.0	IntrinsicReward: -162.78871177276596
Accepted New Salient Event:  SE([6 3])
[DSGTrainer] Adding new SalientEvent  SE([6 3])
Took 17.616181135177612s to update distance table with 20040 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -173.22989195026457
Took 0.08134055137634277s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -173.865287987981
Took 0.09530949592590332s to update distance table with 1001 states and events {SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -169.58491641748697
Took 0.08320856094360352s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -165.5917183989659
Took 0.07509589195251465s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -180.82306088891346
Took 0.09720301628112793s to update distance table with 1001 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -170.61029859934933
Took 0.07960009574890137s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 1.0	IntrinsicReward: -170.52628188976087
Took 0.07992124557495117s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -158.16505642305128
Took 0.0683138370513916s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 1.0	IntrinsicReward: -150.44543433716171
Took 0.08206295967102051s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -178.7361878621159
Took 0.07209897041320801s to update distance table with 1001 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -188.83789441362023
Took 0.08504629135131836s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -178.47225251840428
Took 0.08417439460754395s to update distance table with 1001 states and events {SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 4.0	IntrinsicReward: -154.40453447424807
Took 0.08334207534790039s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -178.69405443192227
Took 0.08271145820617676s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -183.39986563508864
Took 0.07348823547363281s to update distance table with 1001 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 4.0	IntrinsicReward: -182.17661368567497
Took 0.07487988471984863s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 1.0	IntrinsicReward: -175.9928955284704
Took 0.0768880844116211s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -183.33437584945932
Took 0.06590127944946289s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -169.54653849586612
Took 0.07228207588195801s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), SE([4 2]), None}
[RND Rollout] Reward: 1.0	IntrinsicReward: -181.49248584685847
Took 0.0969233512878418s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -160.10173770558322
Took 0.08990240097045898s to update distance table with 1001 states and events {SE([6 6]), SE([1 1]), SE([6 3]), None}
[RND Rollout] Reward: 1.0	IntrinsicReward: -196.94009780697525
Took 0.06778120994567871s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 1.0	IntrinsicReward: -180.13312016963027
Took 0.06717610359191895s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -193.55342613067478
Took 0.07752561569213867s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 6.0	IntrinsicReward: -171.27046665619127
Took 0.06387615203857422s to update distance table with 1001 states and events {SE([6 6]), None, SE([1 1])}
[RND Rollout] Reward: 6.0	IntrinsicReward: -194.94878212176263
Took 0.0820004940032959s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 4.0	IntrinsicReward: -184.2318732933054
Took 0.09168577194213867s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 1.0	IntrinsicReward: -182.09571166697424
Took 0.08413028717041016s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -179.93476798798656
Took 0.085113525390625s to update distance table with 1001 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -185.9774905918166
Took 0.12285304069519043s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -193.0951452224981
Took 0.07579445838928223s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -200.21283992147073
Took 0.08511972427368164s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 4.0	IntrinsicReward: -194.44086169824004
Took 0.07254695892333984s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -182.6321169277653
Took 0.06960821151733398s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 5.0	IntrinsicReward: -185.70755740703316
Took 0.08870196342468262s to update distance table with 1001 states and events {SE([6 6]), None, SE([1 1])}
[RND Rollout] Reward: 7.0	IntrinsicReward: -188.34108717832714
Took 0.0679783821105957s to update distance table with 1001 states and events {SE([6 6]), SE([1 1]), SE([4 2]), None}
[RND Rollout] Reward: 3.0	IntrinsicReward: -175.44151011900976
Took 0.08616948127746582s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -193.19371548993513
Took 0.0886838436126709s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -155.72785411332734
Took 0.061852455139160156s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -211.65245256386697
Took 0.0771036148071289s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 5.0	IntrinsicReward: -216.70219210349023
Took 0.09296560287475586s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -179.42750124214217
Took 0.08753609657287598s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 5.0	IntrinsicReward: -211.35380881465971
Took 0.13269686698913574s to update distance table with 1001 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[RND Rollout] Reward: 8.0	IntrinsicReward: -196.6200019000098
Took 0.10674118995666504s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 2.0	IntrinsicReward: -200.40699431701796
Took 0.10097765922546387s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -170.48391030549828
Took 0.07526659965515137s to update distance table with 1001 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
[RND Rollout] Reward: 4.0	IntrinsicReward: -222.8968154489994
Took 0.0782771110534668s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 5.0	IntrinsicReward: -219.79399115219712
Took 0.07410192489624023s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 7.0	IntrinsicReward: -198.383083159104
Took 0.09877753257751465s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[RND Rollout] Reward: 3.0	IntrinsicReward: -182.17724783763697
Took 0.12413644790649414s to update distance table with 1001 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
[SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])]
================================================================================
[Consolidation] Episode: 90 Step: 0
================================================================================
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[DeepSkillGraphsAgent] Creating chain from SE([1 1]) -> SE([6 6])
Creating classifier of type cnn
Created model-free option goal-option-1 with option_idx=1
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 6])
Planner goal: SE([1 1]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [1 1] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
Updated goal-option-1 on 49 transitions
Took 0.002807140350341797s to update distance table with 50 states and events {SE([6 6]), None, SE([1 1])}
DSG successfully reached SE([6 6])
Took 1.7642974853515625e-05s to add potential edges.
================================================================================
[Consolidation] Episode: 91 Step: 49
================================================================================
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 6])
Planner goal: SE([1 1]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [1 1] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
13.515607143441835
Updated goal-option-1 on 256 transitions
Took 0.019896268844604492s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 92 Step: 305
================================================================================
[Random] Deep skill graphs target event: SE([6 3])
[Random] DSG selected event SE([6 3])
[DeepSkillGraphsAgent] Creating chain from SE([1 1]) -> SE([6 3])
Creating classifier of type cnn
Created model-free option goal-option-2 with option_idx=2
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 3])
Planner goal: SE([1 1]), DSC goal: SE([6 3]) and Goal: SE([6 3])
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [1 1] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 256 transitions
Took 0.015351057052612305s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 93 Step: 561
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00776513 0.00186092 0.00186092 0.00186092 0.96398143 0.02267067]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[DeepSkillGraphsAgent] Creating chain from SE([1 1]) -> SE([4 2])
Creating classifier of type cnn
Created model-free option goal-option-3 with option_idx=3
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-3 on 77 transitions
Took 0.004907846450805664s to update distance table with 78 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 94 Step: 638
================================================================================
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 6])
Planner goal: SE([1 1]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [1 1] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
12.93608400821686
Updated goal-option-1 on 136 transitions
Took 0.011020421981811523s to update distance table with 137 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 95 Step: 774
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-3 on 256 transitions
Took 0.01073312759399414s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 96 Step: 1030
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
11.77299341360728
Updated goal-option-3 on 142 transitions
Took 0.00809478759765625s to update distance table with 143 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[DeepSkillGraphsAgent] Creating chain from SE([4 2]) -> SE([1 1])
Creating classifier of type cnn
Created model-free option goal-option-4 with option_idx=4
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
goal-option-4 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
Updated goal-option-4 on 49 transitions
Took 0.002806425094604492s to update distance table with 50 states and events {SE([1 1]), SE([4 2]), None}
DSG successfully reached SE([1 1])
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
14.520536220735975
Updated goal-option-3 on 29 transitions
Took 0.0011415481567382812s to update distance table with 30 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([1 1])
[Random] DSG selected event SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
17.92513908280267
Updated goal-option-4 on 36 transitions
Took 0.0026960372924804688s to update distance table with 37 states and events {SE([6 3]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 97 Step: 1286
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
7.707083399097125
Updated goal-option-3 on 256 transitions
Took 0.013967037200927734s to update distance table with 257 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 98 Step: 1542
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
11.899632261859047
Updated goal-option-3 on 150 transitions
Took 0.013239145278930664s to update distance table with 151 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 99 Step: 1692
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 3])
Planner goal: SE([1 1]), DSC goal: SE([6 3]) and Goal: SE([6 3])
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [1 1] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
10.629159006807539
Updated goal-option-2 on 37 transitions
Took 0.0019311904907226562s to update distance table with 38 states and events {SE([6 3]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2])] | Probs: [[9.28067087e-07 9.98727884e-01 5.77039869e-08 5.77039869e-08
  5.77039869e-08 1.27101486e-03]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[DeepSkillGraphsAgent] Creating chain from SE([6 3]) -> SE([6 6])
Creating classifier of type cnn
Created model-free option goal-option-5 with option_idx=5
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
10.1145213331495
Updated goal-option-1 on 219 transitions
Took 0.018839120864868164s to update distance table with 220 states and events {SE([6 3]), None}
================================================================================
[Consolidation] Episode: 100 Step: 1948
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 3 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
6.2698511698029264
Updated goal-option-3 on 230 transitions
Took 0.011887311935424805s to update distance table with 231 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
18.84204559524854
Updated goal-option-4 on 26 transitions
Took 0.0015759468078613281s to update distance table with 27 states and events {SE([4 2]), None}
Took 5.4836273193359375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 101 Step: 2204
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1632, 'bonus': 0.02475368857441686, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2178, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 4 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.453175614277522
Updated goal-option-3 on 67 transitions
Took 0.004889726638793945s to update distance table with 68 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6150, 'bonus': 0.012751534261266767, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1221, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
14.922593166430792
Updated goal-option-4 on 59 transitions
Took 0.00347900390625s to update distance table with 60 states and events {SE([6 6]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 102 Step: 2330
================================================================================
[Random] Deep skill graphs target event: SE([6 3])
[Random] DSG selected event SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 3])
Planner goal: SE([1 1]), DSC goal: SE([6 3]) and Goal: SE([6 3])
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [1 1] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
17.7894384543101
Updated goal-option-2 on 256 transitions
Took 0.014575004577636719s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 103 Step: 2586
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 5 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.02622774101439
Updated goal-option-3 on 47 transitions
Took 0.002508878707885742s to update distance table with 48 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([1 1])
[Random] DSG selected event SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6150, 'bonus': 0.012751534261266767, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1221, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
14.956752467723119
Updated goal-option-4 on 29 transitions
Took 0.0017731189727783203s to update distance table with 30 states and events {SE([6 6]), SE([6 3]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 104 Step: 2662
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1640, 'bonus': 0.02469323991623974, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2271, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
3.2692767413126096
Updated goal-option-3 on 84 transitions
Took 0.006493091583251953s to update distance table with 85 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 105 Step: 2746
================================================================================
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 6])
Planner goal: SE([1 1]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [1 1] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 3 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
8.369173200180134
Updated goal-option-1 on 59 transitions
Took 0.003213644027709961s to update distance table with 60 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 106 Step: 2805
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[0.00774045 0.00185501 0.00185501 0.00185501 0.9609179  0.02577662]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1631, 'bonus': 0.024761275912089115, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1250, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 6 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.955677071622773
Updated goal-option-3 on 33 transitions
Took 0.0015037059783935547s to update distance table with 34 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6150, 'bonus': 0.012751534261266767, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1221, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
23.1518283461531
Updated goal-option-4 on 215 transitions
Took 0.015221834182739258s to update distance table with 216 states and events {SE([6 6]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 107 Step: 3053
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1632, 'bonus': 0.02475368857441686, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2178, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 7 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
8.74259207646052
Updated goal-option-3 on 6 transitions
Took 0.0004127025604248047s to update distance table with 7 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6922, 'bonus': 0.012019439133170468, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2805, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.485404272874197
Updated goal-option-1 on 250 transitions
Took 0.016660451889038086s to update distance table with 251 states and events {SE([1 1]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 108 Step: 3309
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1632, 'bonus': 0.02475368857441686, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2178, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 8 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.854908015905765
Updated goal-option-3 on 46 transitions
Took 0.002838611602783203s to update distance table with 47 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6922, 'bonus': 0.012019439133170468, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2805, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 4 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.1337251283905725
Updated goal-option-1 on 43 transitions
Took 0.002553701400756836s to update distance table with 44 states and events {SE([6 6]), SE([4 2]), None}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 109 Step: 3398
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1645, 'bonus': 0.024655683636076897, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2838, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.593435483674208
Updated goal-option-3 on 96 transitions
Took 0.005558967590332031s to update distance table with 97 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 110 Step: 3494
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
3.0765398758830447
Updated goal-option-3 on 256 transitions
Took 0.015397071838378906s to update distance table with 257 states and events {None, SE([1 1])}
Took 5.245208740234375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 111 Step: 3750
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 9 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.735879432895909
Updated goal-option-3 on 47 transitions
Took 0.0031092166900634766s to update distance table with 48 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 5 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.914372703203789
Updated goal-option-1 on 111 transitions
Took 0.006457328796386719s to update distance table with 112 states and events {SE([6 6]), SE([4 2]), None}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 112 Step: 3908
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
3.0923477296034494
Updated goal-option-3 on 82 transitions
Took 0.005362987518310547s to update distance table with 83 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 113 Step: 3990
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probs: [[9.97944064e-04 2.39158335e-04 2.39158335e-04 2.39158335e-04
  9.94961310e-01 3.32327076e-03]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([4 2])
Planner goal: SE([1 1]), DSC goal: SE([4 2]) and Goal: SE([4 2])
False
False
Rolling out DSC with goal vertex SE([4 2])
Rolling out goal-option-3, from [1 1] targeting {'count': 1632, 'bonus': 0.02475368857441686, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2178, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 10 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.6966031358028069
Updated goal-option-3 on 182 transitions
Expanding goal-option-3's pessimistic classifier to include SE([1 1])
Case 3: Adding edge from goal-option-3 to SE([4 2])
Case 2: Adding edge from SE([1 1]) to goal-option-3
Adding edge from SE([1 1]) to goal-option-3
Expanding goal-option-3's pessimistic classifier to include SE([1 1])
Took 0.009235143661499023s to update distance table with 183 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6917, 'bonus': 0.012023782514634523, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 774, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.511015177435345
Updated goal-option-1 on 74 transitions
Took 0.0050394535064697266s to update distance table with 75 states and events {SE([4 2]), None}
================================================================================
[Consolidation] Episode: 114 Step: 4246
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4246, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.42204980977944
Updated goal-option-3 on 256 transitions
Took 0.016090869903564453s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 115 Step: 4502
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4502, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 11 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.724548079735703
Updated goal-option-3 on 55 transitions
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6922, 'bonus': 0.012019439133170468, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2805, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 6 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.2660825146983066
Updated goal-option-1 on 149 transitions
Took 0.013195037841796875s to update distance table with 205 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 116 Step: 4706
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4706, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1631, 'bonus': 0.024761275912089115, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1250, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.0570191468602865
Updated goal-option-3 on 180 transitions
Took 0.011009931564331055s to update distance table with 181 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 117 Step: 4886
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4886, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1640, 'bonus': 0.02469323991623974, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2271, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 12 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.8549532017510648
Updated goal-option-3 on 40 transitions
Took 0.0027077198028564453s to update distance table with 41 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 7 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.9468466184205482
Updated goal-option-1 on 94 transitions
Took 0.005617856979370117s to update distance table with 95 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 118 Step: 5020
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5020, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1645, 'bonus': 0.024655683636076897, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2838, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 13 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.0874602785881824
Updated goal-option-3 on 46 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
6.471925695737203
Updated goal-option-2 on 210 transitions
Took 0.014781475067138672s to update distance table with 257 states and events {SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 119 Step: 5276
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5276, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1682, 'bonus': 0.024382992454708537, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5066, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.119511307803569
Updated goal-option-3 on 71 transitions
Took 0.0033109188079833984s to update distance table with 72 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 120 Step: 5347
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5347, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1647, 'bonus': 0.024640709031819093, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3059, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 14 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.0396747538098343
Updated goal-option-3 on 50 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
8.854330544670423
Updated goal-option-2 on 79 transitions
Took 0.00744175910949707s to update distance table with 130 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6])] | Probs: [[9.29248175e-07 9.99998897e-01 5.77774228e-08 5.77774228e-08
  5.77774228e-08]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 8 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.1370207923981877
Updated goal-option-1 on 13 transitions
Took 0.00087738037109375s to update distance table with 14 states and events {SE([6 6]), SE([6 3]), None}
DSG successfully reached SE([6 6])
Adding edge from SE([4 2]) to goal-option-3
Took 0.016991853713989258s to add potential edges.
================================================================================
[Consolidation] Episode: 121 Step: 5489
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5489, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1631, 'bonus': 0.024761275912089115, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1250, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 15 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.312760727447376
Updated goal-option-3 on 23 transitions
Took 0.0013494491577148438s to update distance table with 24 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6150, 'bonus': 0.012751534261266767, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1221, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-4 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
5.87540464202563
Updated goal-option-4 on 100 transitions
Took 0.005848407745361328s to update distance table with 101 states and events {SE([1 1]), SE([4 2]), None}
DSG successfully reached SE([1 1])
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'count': 6631, 'bonus': 0.012280342685287913, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5612, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.4883188641574394
Updated goal-option-3 on 133 transitions
Took 0.0070629119873046875s to update distance table with 134 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 122 Step: 5745
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([1 1]) and dsc goal vertex to SE([6 6])
Planner goal: SE([1 1]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [1 1] targeting {'count': 6926, 'bonus': 0.012015967814936865, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3908, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.2356595871349176
Updated goal-option-1 on 256 transitions
Took 0.0160067081451416s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 123 Step: 6001
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6001, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1640, 'bonus': 0.02469323991623974, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2271, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 16 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.1500461311390002
Updated goal-option-3 on 18 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
5.5633448233207075
Updated goal-option-2 on 102 transitions
Took 0.006679058074951172s to update distance table with 121 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 124 Step: 6121
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6121, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1675, 'bonus': 0.024433888871261047, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4926, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 17 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.5135266667320615
Updated goal-option-3 on 45 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
7.5837306276415335
Updated goal-option-2 on 211 transitions
Took 0.01461482048034668s to update distance table with 257 states and events {SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 125 Step: 6377
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6377, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1690, 'bonus': 0.024325212770525996, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5512, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 18 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.5999787767497142
Updated goal-option-3 on 27 transitions
Took 0.0006821155548095703s to update distance table with 28 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6150, 'bonus': 0.012751534261266767, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1221, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-4 reached term set 3 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
5.842080099113059
Updated goal-option-4 on 88 transitions
Took 0.0050508975982666016s to update distance table with 89 states and events {SE([1 1]), SE([4 2]), None}
DSG successfully reached SE([1 1])
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'count': 6859, 'bonus': 0.012074512308976935, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6492, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1574490224999008
Updated goal-option-3 on 54 transitions
Took 0.0028748512268066406s to update distance table with 55 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 126 Step: 6546
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6546, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1701, 'bonus': 0.024246432248443597, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6404, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 19 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.1882442674151175
Updated goal-option-3 on 31 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 3 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
7.365027133470927
Updated goal-option-2 on 70 transitions
Took 0.00572967529296875s to update distance table with 102 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'count': 6932, 'bonus': 0.012010766472301584, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5489, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.6498057700338817
Updated goal-option-1 on 155 transitions
Took 0.008842229843139648s to update distance table with 156 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 127 Step: 6802
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6802, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1251607013086902
Updated goal-option-3 on 256 transitions
Took 0.01598381996154785s to update distance table with 257 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 128 Step: 7058
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7058, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1642, 'bonus': 0.0246781968201642, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2633, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.210141141485009
Updated goal-option-3 on 137 transitions
Took 0.007743120193481445s to update distance table with 138 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 129 Step: 7195
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7195, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1645, 'bonus': 0.024655683636076897, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2838, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8401676692565283
Updated goal-option-3 on 153 transitions
Took 0.00741887092590332s to update distance table with 154 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 130 Step: 7348
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7348, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 20 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.037110178493986
Updated goal-option-3 on 29 transitions
Took 0.0011506080627441406s to update distance table with 30 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 3])
[Random] DSG selected event SE([6 3])
[Graph Consolidation] From (4, 2) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
7.295181397526037
Updated goal-option-2 on 227 transitions
Took 0.012417793273925781s to update distance table with 228 states and events {SE([1 1]), SE([4 2]), None}
Took 0.0007092952728271484s to add potential edges.
================================================================================
[Consolidation] Episode: 131 Step: 7604
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7604, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1626, 'bonus': 0.02479931753217214, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.794636831437267
Updated goal-option-3 on 219 transitions
Took 0.011728525161743164s to update distance table with 220 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 132 Step: 7823
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7823, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1697, 'bonus': 0.02427499107135649, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6019, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.9827094527858273
Updated goal-option-3 on 256 transitions
Took 0.014123201370239258s to update distance table with 257 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 133 Step: 8079
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.19805626 0.04746439 0.04746439 0.04746439 0.65955057]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8079, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1660, 'bonus': 0.0245440346836908, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3797, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3837450297492924
Updated goal-option-3 on 25 transitions
Took 0.0013813972473144531s to update distance table with 26 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 134 Step: 8104
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.32219004 0.04011732 0.04011732 0.04011732 0.557458  ]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8104, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1640, 'bonus': 0.02469323991623974, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2271, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1084986836241002
Updated goal-option-3 on 69 transitions
Took 0.0037887096405029297s to update distance table with 70 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 135 Step: 8173
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.32219004 0.04011732 0.04011732 0.04011732 0.557458  ]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8173, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1675, 'bonus': 0.024433888871261047, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4926, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 21 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.054231116655229
Updated goal-option-3 on 234 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
6.493552188740836
Updated goal-option-2 on 22 transitions
Took 0.016085386276245117s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 136 Step: 8429
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.32219004 0.04011732 0.04011732 0.04011732 0.557458  ]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8429, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1645, 'bonus': 0.024655683636076897, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2838, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.798347868439224
Updated goal-option-3 on 161 transitions
Took 0.008724689483642578s to update distance table with 162 states and events {SE([6 6]), None, SE([1 1])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 137 Step: 8590
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8590, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1670, 'bonus': 0.024470439211619822, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4557, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 22 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
0.9540495422205639
Updated goal-option-3 on 91 transitions
Took 0.004323482513427734s to update distance table with 92 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99999981e-01 1.30828924e-09 1.38879439e-11 1.38879439e-11
  1.38879439e-11 1.75687712e-08]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6859, 'bonus': 0.012074512308976935, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6492, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
5.620194925711705
Updated goal-option-4 on 80 transitions
Took 0.007353782653808594s to update distance table with 81 states and events {SE([6 6]), SE([6 3]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 138 Step: 8761
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.32219004 0.04011732 0.04011732 0.04011732 0.557458  ]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8761, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1707, 'bonus': 0.024203782378574807, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6577, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0300361389520936
Updated goal-option-3 on 125 transitions
Took 0.005606412887573242s to update distance table with 126 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 139 Step: 8886
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.32219004 0.04011732 0.04011732 0.04011732 0.557458  ]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8886, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1667, 'bonus': 0.02449244830545123, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4172, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.1053643564335425
Updated goal-option-3 on 248 transitions
Took 0.013765573501586914s to update distance table with 249 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
[Episode=140 Seed=18] Took 0.04526090621948242s to save gc logs
[SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])]
================================================================================
[Consolidation] Episode: 140 Step: 9134
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.20559691 0.02559979 0.02559979 0.02559979 0.71760372]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9134, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1698, 'bonus': 0.024267841905814505, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6166, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8718174732092654
Updated goal-option-3 on 148 transitions
Took 0.007892608642578125s to update distance table with 149 states and events {SE([6 6]), None, SE([1 1])}
Took 0.0006771087646484375s to add potential edges.
================================================================================
[Consolidation] Episode: 141 Step: 9282
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9282, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1698, 'bonus': 0.024267841905814505, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6166, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.744578869684654
Updated goal-option-3 on 130 transitions
Took 0.0070993900299072266s to update distance table with 131 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 142 Step: 9412
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.20559691 0.02559979 0.02559979 0.02559979 0.71760372]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9412, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1686, 'bonus': 0.024354051207174154, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5397, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.1250518232152085
Updated goal-option-3 on 101 transitions
Took 0.0058634281158447266s to update distance table with 102 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 143 Step: 9513
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.20559691 0.02559979 0.02559979 0.02559979 0.71760372]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9513, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1701, 'bonus': 0.024246432248443597, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6404, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 23 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.1866912531880704
Updated goal-option-3 on 173 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2600, 'bonus': 0.019611613513818404, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6647, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
3.69947599605018
Updated goal-option-2 on 83 transitions
Took 0.014868736267089844s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 144 Step: 9769
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.20559691 0.02559979 0.02559979 0.02559979 0.71760372]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9769, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1707, 'bonus': 0.024203782378574807, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6577, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 24 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.558759721023428
Updated goal-option-3 on 88 transitions
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8180310503728148
Updated goal-option-1 on 168 transitions
Took 0.019048213958740234s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 145 Step: 10025
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10025, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1682, 'bonus': 0.024382992454708537, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5066, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3148172077563431
Updated goal-option-3 on 256 transitions
Took 0.013599395751953125s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 146 Step: 10281
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10281, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1670, 'bonus': 0.024470439211619822, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 4557, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 25 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.7169292059383896
Updated goal-option-3 on 90 transitions
Took 0.004849672317504883s to update distance table with 91 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99996272e-01 1.30828439e-09 1.38878924e-11 1.38878924e-11
  1.38878924e-11 3.72663928e-06]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6859, 'bonus': 0.012074512308976935, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6492, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
6.161377857791052
Updated goal-option-4 on 166 transitions
Took 0.009671211242675781s to update distance table with 167 states and events {SE([4 2]), None}
================================================================================
[Consolidation] Episode: 147 Step: 10537
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10537, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1660, 'bonus': 0.0245440346836908, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3797, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 26 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.039423563586169
Updated goal-option-3 on 41 transitions
Took 0.0024132728576660156s to update distance table with 42 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99996272e-01 1.30828439e-09 1.38878924e-11 1.38878924e-11
  1.38878924e-11 3.72663928e-06]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.992639321088791
Updated goal-option-4 on 16 transitions
Took 0.0009942054748535156s to update distance table with 17 states and events {SE([6 6]), SE([4 2]), None}
================================================================================
[Consolidation] Episode: 148 Step: 10594
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10594, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1697, 'bonus': 0.02427499107135649, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6019, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2597007322663496
Updated goal-option-3 on 136 transitions
Took 0.006863117218017578s to update distance table with 137 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 149 Step: 10730
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.37899165 0.02001211 0.02001211 0.02001211 0.56097202]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10730, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1647, 'bonus': 0.024640709031819093, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3059, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 27 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
0.887093256538113
Updated goal-option-3 on 103 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2597, 'bonus': 0.019622937706955574, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5476, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
3.302156905333201
Updated goal-option-2 on 153 transitions
Took 0.01428985595703125s to update distance table with 257 states and events {SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 150 Step: 10986
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.37899165 0.02001211 0.02001211 0.02001211 0.56097202]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10986, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1701, 'bonus': 0.024246432248443597, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6404, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2229035597264282
Updated goal-option-3 on 217 transitions
Took 0.012953519821166992s to update distance table with 218 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
DSG successfully reached SE([6 6])
Took 0.0006945133209228516s to add potential edges.
================================================================================
[Consolidation] Episode: 151 Step: 11203
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11203, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1660, 'bonus': 0.0245440346836908, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 3797, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 28 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.629884087720619
Updated goal-option-3 on 19 transitions
Took 0.0011479854583740234s to update distance table with 20 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99996272e-01 1.30828439e-09 1.38878924e-11 1.38878924e-11
  1.38878924e-11 3.72663928e-06]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'count': 6859, 'bonus': 0.012074512308976935, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6492, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-4 reached term set 4 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.2426338113990485
Updated goal-option-4 on 73 transitions
Took 0.004571199417114258s to update distance table with 74 states and events {SE([1 1]), SE([4 2]), None}
DSG successfully reached SE([1 1])
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.37899165 0.02001211 0.02001211 0.02001211 0.56097202]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'count': 7445, 'bonus': 0.011589578700395143, 'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11295, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1701, 'bonus': 0.024246432248443597, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6404, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 29 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.519278822200639
Updated goal-option-3 on 51 transitions
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 9 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.9843799543380736
Updated goal-option-1 on 57 transitions
Took 0.0078029632568359375s to update distance table with 109 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 152 Step: 11403
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11403, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1707, 'bonus': 0.024203782378574807, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6577, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.5884544782975845
Updated goal-option-3 on 114 transitions
Took 0.00579833984375s to update distance table with 115 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 153 Step: 11517
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.37899165 0.02001211 0.02001211 0.02001211 0.56097202]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11517, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1701, 'bonus': 0.024246432248443597, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6404, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3342213454260223
Updated goal-option-3 on 256 transitions
Took 0.014327287673950195s to update distance table with 257 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 154 Step: 11773
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11773, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1686, 'bonus': 0.024354051207174154, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5397, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0405590768610493
Updated goal-option-3 on 81 transitions
Took 0.004582405090332031s to update distance table with 82 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 155 Step: 11854
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11854, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1751, 'bonus': 0.023897745234617563, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11346, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 30 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
0.7027191480000813
Updated goal-option-3 on 208 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.6769469683369005
Updated goal-option-2 on 48 transitions
Took 0.016832590103149414s to update distance table with 257 states and events {SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 156 Step: 12110
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12110, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1762, 'bonus': 0.02382303276028985, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12062, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.905697732515957
Updated goal-option-3 on 151 transitions
Took 0.009662866592407227s to update distance table with 152 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 157 Step: 12261
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12261, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1697, 'bonus': 0.02427499107135649, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6019, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 31 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.365813039824472
Updated goal-option-3 on 160 transitions
Deleting edge from SE([4 2]) to goal-option-3
Took 0.008059024810791016s to update distance table with 161 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99996272e-01 1.30828439e-09 1.38878924e-11 1.38878924e-11
  1.38878924e-11 3.72663928e-06]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.2877218877827685
Updated goal-option-4 on 96 transitions
Took 0.006196022033691406s to update distance table with 97 states and events {SE([4 2]), None}
================================================================================
[Consolidation] Episode: 158 Step: 12517
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12517, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1723, 'bonus': 0.02409114054616049, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8681, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 32 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3810771416153822
Updated goal-option-3 on 201 transitions
Took 0.011460542678833008s to update distance table with 202 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[9.99996272e-01 1.30828439e-09 1.38878924e-11 1.38878924e-11
  1.38878924e-11 3.72663928e-06]]
[BoltzmannClosest] DSG selected event: SE([1 1])
[Graph Consolidation] From (4, 2) to [1 1]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([1 1])
Planner goal: SE([4 2]), DSC goal: SE([1 1]) and Goal: SE([1 1])
False
False
Rolling out DSC with goal vertex SE([1 1])
Rolling out goal-option-4, from [4 2] targeting {'player_x': 1, 'player_y': 1, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.948810288310051
Updated goal-option-4 on 55 transitions
Took 0.0032646656036376953s to update distance table with 56 states and events {SE([4 2]), None}
================================================================================
[Consolidation] Episode: 159 Step: 12773
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12773, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1732, 'bonus': 0.024028466566610635, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9857, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 33 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.576765678387737
Updated goal-option-3 on 198 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2597, 'bonus': 0.019622937706955574, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5476, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.926301230986913
Updated goal-option-2 on 48 transitions
Took 0.017128705978393555s to update distance table with 247 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
================================================================================
[Consolidation] Episode: 160 Step: 13019
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13019, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1743, 'bonus': 0.02395252523826518, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10578, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 34 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3534350806758517
Updated goal-option-3 on 219 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 4 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
6.6251638346549235
Updated goal-option-2 on 25 transitions
Took 0.014055252075195312s to update distance table with 245 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2])] | Probs: [[9.28067087e-07 9.98727884e-01 5.77039869e-08 5.77039869e-08
  5.77039869e-08 1.27101486e-03]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
4.116748612324397
Updated goal-option-1 on 12 transitions
Took 0.0010318756103515625s to update distance table with 13 states and events {SE([6 3])}
Adding edge from SE([4 2]) to goal-option-3
Took 0.023270845413208008s to add potential edges.
================================================================================
[Consolidation] Episode: 161 Step: 13275
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13275, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1751, 'bonus': 0.023897745234617563, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11346, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.266992272833983
Updated goal-option-3 on 78 transitions
Took 0.00536346435546875s to update distance table with 79 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 162 Step: 13353
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13353, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1743, 'bonus': 0.02395252523826518, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10578, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0441534085795772
Updated goal-option-3 on 71 transitions
Took 0.004872560501098633s to update distance table with 72 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 163 Step: 13424
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13424, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1723, 'bonus': 0.02409114054616049, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8681, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0961633980467098
Updated goal-option-3 on 85 transitions
Took 0.003576517105102539s to update distance table with 86 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 164 Step: 13509
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13509, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1717, 'bonus': 0.02413319668619763, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 8407, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 35 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3859065407798405
Updated goal-option-3 on 49 transitions
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [4 2] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 10 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.080504067334128
Updated goal-option-1 on 193 transitions
Creating goal-option-1-0 with parent goal-option-1
Creating classifier of type cnn
Created model-free option goal-option-1-0 with option_idx=6
Case 3: Adding edge from goal-option-1 to SE([6 6])
Adding edge from SE([6 6]) to goal-option-1
Adding edge from SE([6 6]) to goal-option-1
Adding edge from SE([6 6]) to goal-option-1
Adding edge from SE([6 3]) to goal-option-1
Took 0.013687372207641602s to update distance table with 243 states and events {None, SE([6 6]), SE([1 1]), SE([6 3]), SE([4 2])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 165 Step: 13751
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.2216586  0.01170437 0.01170437 0.75493267]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13751, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1791, 'bonus': 0.023629373500277863, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13558, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2031982509220172
Updated goal-option-3 on 256 transitions
Took 0.014104366302490234s to update distance table with 257 states and events {SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 166 Step: 14007
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.2216586  0.01170437 0.01170437 0.75493267]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14007, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1707, 'bonus': 0.024203782378574807, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6577, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 36 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
0.8904679327553308
Updated goal-option-3 on 22 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2571, 'bonus': 0.019721909504608663, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 1729, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 5 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.06351051479578
Updated goal-option-2 on 59 transitions
Took 0.005792856216430664s to update distance table with 82 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6])] | Probs: [[9.29248229e-07 9.99998955e-01 5.77774262e-08 5.77774262e-08]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'count': 6922, 'bonus': 0.012019439133170468, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 2805, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 11 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.4442920116362745
Updated goal-option-1 on 23 transitions
Took 0.0016913414001464844s to update distance table with 24 states and events {SE([6 6]), SE([6 3]), None}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 167 Step: 14111
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14111, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1793, 'bonus': 0.02361619114304532, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14029, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6160068903858873
Updated goal-option-3 on 132 transitions
Took 0.007493019104003906s to update distance table with 133 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 168 Step: 14243
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14243, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1768, 'bonus': 0.023782574707724703, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12421, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.18439956072605
Updated goal-option-3 on 101 transitions
Took 0.005602598190307617s to update distance table with 102 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 169 Step: 14344
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14344, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1732, 'bonus': 0.024028466566610635, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9857, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 37 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.4470368930626483
Updated goal-option-3 on 117 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2600, 'bonus': 0.019611613513818404, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6647, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 6 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
4.798865943505214
Updated goal-option-2 on 68 transitions
Took 0.011520624160766602s to update distance table with 186 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2])] | Probs: [[9.28067141e-07 9.98727942e-01 5.77039902e-08 5.77039902e-08
  1.27101494e-03]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'count': 6959, 'bonus': 0.011987443735004533, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14111, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.96508010673797
Updated goal-option-1 on 71 transitions
Took 0.004235506057739258s to update distance table with 72 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 170 Step: 14600
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.2216586  0.01170437 0.01170437 0.75493267]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14600, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1732, 'bonus': 0.024028466566610635, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9857, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 38 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.2838465289204521
Updated goal-option-3 on 28 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2600, 'bonus': 0.019611613513818404, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6647, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 7 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.588943034792558
Updated goal-option-2 on 13 transitions
Took 0.00261688232421875s to update distance table with 42 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2])] | Probs: [[9.28067141e-07 9.98727942e-01 5.77039902e-08 5.77039902e-08
  1.27101494e-03]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1, from [6 3] targeting {'count': 6958, 'bonus': 0.01198830511852413, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 13751, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 12 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.301016092631552
Updated goal-option-1 on 39 transitions
Took 0.0027260780334472656s to update distance table with 40 states and events {SE([6 6]), SE([6 3]), None}
DSG successfully reached SE([6 6])
Adding edge from SE([6 6]) to goal-option-1
Adding edge from goal-option-1 to SE([6 6])
Adding edge from goal-option-1 to SE([6 6])
Adding edge from goal-option-1 to SE([6 6])
Took 0.17158102989196777s to add potential edges.
================================================================================
[Consolidation] Episode: 171 Step: 14680
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14680, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1729, 'bonus': 0.024049303512194094, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9686, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1194695785031683
Updated goal-option-3 on 74 transitions
Took 0.00417017936706543s to update distance table with 75 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 172 Step: 14754
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14754, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1732, 'bonus': 0.024028466566610635, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9857, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 39 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.4025412887766742
Updated goal-option-3 on 18 transitions
Took 0.0011489391326904297s to update distance table with 19 states and events {SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[Random] Deep skill graphs target event: SE([6 6])
[Random] DSG selected event SE([6 6])
[Graph Consolidation] From (4, 2) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2835, 'bonus': 0.0187812056606337, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14641, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 8 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.8690585777873083
Updated goal-option-2 on 71 transitions
True
False
[Planner] Rolling out from {'count': 2844, 'bonus': 0.018751465015433733, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14843, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 6])
Got plan [goal-option-1] and chose to execute goal-option-1
Rolling out goal-option-1, from [6 3] targeting {'count': 6915, 'bonus': 0.012025521186108383, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 49, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 13 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.3978110103196992
Updated goal-option-1 on 34 transitions
Deleting edge from SE([6 3]) to goal-option-1
Took 0.006117582321166992s to update distance table with 106 states and events {SE([6 6]), SE([6 3]), SE([4 2]), None}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 173 Step: 14877
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14877, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1793, 'bonus': 0.02361619114304532, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14029, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1919794111085413
Updated goal-option-3 on 256 transitions
Took 0.013887405395507812s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 174 Step: 15133
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15133, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1751, 'bonus': 0.023897745234617563, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11346, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3173459107424579
Updated goal-option-3 on 256 transitions
Deleting edge from SE([4 2]) to goal-option-3
Took 0.01444101333618164s to update distance table with 257 states and events {None, SE([1 1])}
================================================================================
[Consolidation] Episode: 175 Step: 15389
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15389, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1749, 'bonus': 0.023911404992940522, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 11222, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2488007216817802
Updated goal-option-3 on 43 transitions
Took 0.0031943321228027344s to update distance table with 44 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 176 Step: 15432
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15432, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1779, 'bonus': 0.023708933905523488, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12718, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.80009319647613
Updated goal-option-3 on 67 transitions
Took 0.0047304630279541016s to update distance table with 68 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 177 Step: 15499
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15499, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1744, 'bonus': 0.023945657130528784, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10833, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 40 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.2820280834706754
Updated goal-option-3 on 33 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2835, 'bonus': 0.0187812056606337, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14641, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.428068404910208
Updated goal-option-2 on 219 transitions
Took 0.014309167861938477s to update distance table with 253 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 178 Step: 15751
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15751, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1744, 'bonus': 0.023945657130528784, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 10833, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3813785889776804
Updated goal-option-3 on 143 transitions
Took 0.008370161056518555s to update distance table with 144 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 179 Step: 15894
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15894, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1810, 'bonus': 0.02350502473611342, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15532, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2080498226448972
Updated goal-option-3 on 184 transitions
Took 0.011050224304199219s to update distance table with 185 states and events {SE([6 6]), SE([6 3]), None, SE([1 1])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 180 Step: 16078
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 6])
Planner goal: SE([4 2]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16078, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1729, 'bonus': 0.024049303512194094, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 9686, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.588067411046641
Updated goal-option-3 on 197 transitions
Took 0.010443687438964844s to update distance table with 198 states and events {SE([6 6]), None, SE([1 1])}
DSG successfully reached SE([6 6])
Adding edge from SE([4 2]) to goal-option-3
Took 0.19062376022338867s to add potential edges.
================================================================================
[Consolidation] Episode: 181 Step: 16275
================================================================================
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16275, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1810, 'bonus': 0.02350502473611342, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15532, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.361984531700765
Updated goal-option-3 on 152 transitions
Took 0.007799625396728516s to update distance table with 153 states and events {SE([6 6]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 182 Step: 16427
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.21909424 0.01156896 0.01156896 0.01156896 0.74619888]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16427, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1762, 'bonus': 0.02382303276028985, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12062, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 41 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3361920271164331
Updated goal-option-3 on 109 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2844, 'bonus': 0.018751465015433733, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14843, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.6253470147168763
Updated goal-option-2 on 147 transitions
Took 0.018747806549072266s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 183 Step: 16683
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.11117095 0.00587022 0.00587022 0.00587022 0.87121838]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16683, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1779, 'bonus': 0.023708933905523488, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12718, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 42 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
0.8508500339406909
Updated goal-option-3 on 77 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'count': 2600, 'bonus': 0.019611613513818404, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 6647, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 9 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
3.191728202626109
Updated goal-option-2 on 117 transitions
Took 0.011916637420654297s to update distance table with 195 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6])] | Probs: [[2.70471044e-06 9.99997122e-01 5.77773203e-08 5.77773203e-08
  5.77773203e-08]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1-0, from [6 3] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1-0 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
Updated goal-option-1-0 on 2 transitions
Rolling out goal-option-1, from [6 2] targeting {'count': 6959, 'bonus': 0.011987443735004533, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14111, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.9561661820520053
Updated goal-option-1 on 60 transitions
Took 0.0042705535888671875s to update distance table with 63 states and events {SE([6 3]), None}
================================================================================
[Consolidation] Episode: 184 Step: 16939
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 3])] | Probs: [[0.11117095 0.00587022 0.00587022 0.00587022 0.87121838]]
[BoltzmannClosest] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Revised planner goal vertex to SE([4 2]) and dsc goal vertex to SE([6 3])
Planner goal: SE([4 2]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16939, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1814, 'bonus': 0.023479095302146476, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 16760, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 43 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3612802786038334
Updated goal-option-3 on 19 transitions
False
False
Rolling out DSC with goal vertex SE([6 3])
Rolling out goal-option-2, from [4 2] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 10 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.313951878278863
Updated goal-option-2 on 231 transitions
Creating goal-option-2-0 with parent goal-option-2
Creating classifier of type cnn
Created model-free option goal-option-2-0 with option_idx=7
Case 3: Adding edge from goal-option-2 to SE([6 3])
Adding edge from goal-option-2 to goal-option-1
Took 0.013752460479736328s to update distance table with 251 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([4 2])] | Probs: [[1.74424148e-04 9.99825576e-01]]
[BoltzmannClosest] DSG selected event: SE([4 2])
[DeepSkillGraphsAgent] Creating chain from SE([6 6]) -> SE([1 1])
Creating classifier of type cnn
Created model-free option goal-option-6 with option_idx=8
[Graph Consolidation] From (6, 3) to [4 2]
Revised planner goal vertex to SE([6 6]) and dsc goal vertex to SE([1 1])
Planner goal: SE([6 6]), DSC goal: SE([1 1]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'count': 2892, 'bonus': 0.018595200082640047, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17189, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 6])
Got plan [goal-option-1] and chose to execute goal-option-1
Rolling out goal-option-1, from [6 3] targeting {'count': 6932, 'bonus': 0.012010766472301584, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 5489, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.445996036132177
Updated goal-option-1 on 6 transitions
Deleting edge from goal-option-2 to goal-option-1
Took 0.00042939186096191406s to update distance table with 7 states and events {SE([6 3]), None}
================================================================================
[Consolidation] Episode: 185 Step: 17195
================================================================================
Candidate events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probabilities: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
[CompetenceConnected] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Planner goal: SE([6 6]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17195, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 6])
Got plan [goal-option-1] and chose to execute goal-option-1
Rolling out goal-option-1, from [1 1] targeting {'count': 6959, 'bonus': 0.011987443735004533, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14111, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1 reached term set 14 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.2462957708608537
Updated goal-option-1 on 182 transitions
Took 0.009786367416381836s to update distance table with 183 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([6 6])
================================================================================
[Consolidation] Episode: 186 Step: 17377
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6])] | Probs: [[0.86325168 0.04558277 0.04558277 0.04558277]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17377, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 3])
Got plan [goal-option-2] and chose to execute goal-option-2
Rolling out goal-option-2, from [1 1] targeting {'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3916273545178157
Updated goal-option-2 on 256 transitions
Took 0.0163881778717041s to update distance table with 257 states and events {SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 187 Step: 17633
================================================================================
Candidate events: [SE([4 2]), SE([6 3])] | Probabilities: [0.5 0.5]
[CompetenceConnected] DSG selected event: SE([4 2])
[Graph Consolidation] From (1, 1) to [4 2]
Planner goal: SE([4 2]), DSC goal: SE([4 2]) and Goal: SE([4 2])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17633, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([4 2])
Got plan [goal-option-3] and chose to execute goal-option-3
Rolling out goal-option-3, from [1 1] targeting {'count': 1779, 'bonus': 0.023708933905523488, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 12718, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-3 reached term set 44 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.8629728454855545
Updated goal-option-3 on 143 transitions
Took 0.008189916610717773s to update distance table with 144 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
DSG successfully reached SE([4 2])
[CompetenceConnected] DSG selected event: SE([6 3])
[Graph Consolidation] From (4, 2) to [6 3]
Planner goal: SE([6 3]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'count': 1822, 'bonus': 0.023427492832506086, 'player_pos': (4, 2), 'player_x': 4, 'player_y': 2, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17776, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 3])
Got plan [goal-option-2] and chose to execute goal-option-2
Rolling out goal-option-2, from [4 2] targeting {'count': 2835, 'bonus': 0.0187812056606337, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14641, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 11 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.4238265360797848
Updated goal-option-2 on 70 transitions
Took 0.004857301712036133s to update distance table with 71 states and events {SE([6 3]), SE([4 2]), None}
DSG successfully reached SE([6 3])
Unconnected Events: [SE([1 1]), SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2])] | Probs: [[2.66341732e-06 9.84730055e-01 5.68952275e-08 5.68952275e-08
  5.68952275e-08 1.52671106e-02]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (6, 3) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1-0, from [6 3] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
28.933031658331554
Updated goal-option-1-0 on 43 transitions
Took 0.002584218978881836s to update distance table with 44 states and events {SE([6 3]), None}
================================================================================
[Consolidation] Episode: 188 Step: 17889
================================================================================
Unconnected Events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6])] | Probs: [[0.86325168 0.04558277 0.04558277 0.04558277]]
[BoltzmannClosest] DSG selected event: SE([6 6])
[Graph Consolidation] From (1, 1) to [6 6]
Revised planner goal vertex to SE([6 3]) and dsc goal vertex to SE([6 6])
Planner goal: SE([6 3]), DSC goal: SE([6 6]) and Goal: SE([6 6])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17889, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 3])
Got plan [goal-option-2] and chose to execute goal-option-2
Rolling out goal-option-2, from [1 1] targeting {'count': 2892, 'bonus': 0.018595200082640047, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17189, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-2 reached term set 12 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.494006783293005
Updated goal-option-2 on 40 transitions
False
False
Rolling out DSC with goal vertex SE([6 6])
Rolling out goal-option-1-0, from [6 3] targeting {'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
goal-option-1-0 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
13.160197774569193
Updated goal-option-1-0 on 4 transitions
Rolling out goal-option-1, from [6 3] targeting {'count': 6972, 'bonus': 0.01197626261913259, 'player_pos': (6, 6), 'player_x': 6, 'player_y': 6, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': True, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 17377, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
1.724771218167411
Updated goal-option-1 on 212 transitions
Took 0.01789546012878418s to update distance table with 257 states and events {SE([6 3]), SE([4 2]), None, SE([1 1])}
================================================================================
[Consolidation] Episode: 189 Step: 18145
================================================================================
Candidate events: [SE([6 6]), SE([6 6]), SE([6 6]), SE([6 6]), SE([4 2]), SE([6 3])] | Probabilities: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
[CompetenceConnected] DSG selected event: SE([6 3])
[Graph Consolidation] From (1, 1) to [6 3]
Planner goal: SE([6 3]), DSC goal: SE([6 3]) and Goal: SE([6 3])
True
False
[Planner] Rolling out from {'player_pos': (1, 1), 'player_x': 1, 'player_y': 1, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18145, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False} targeting SE([6 3])
Got plan [goal-option-2] and chose to execute goal-option-2
Rolling out goal-option-2, from [1 1] targeting {'count': 2835, 'bonus': 0.0187812056606337, 'player_pos': (6, 3), 'player_x': 6, 'player_y': 3, 'room_number': 1, 'jumping': 0, 'dead': 0, 'falling': 0, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 14641, 'has_key': False, 'door_open': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
2.3608082018792627
Updated goal-option-2 on 229 transitions
Took 0.016290664672851562s to update distance table with 230 states and events {SE([6 6]), SE([4 2]), None, SE([1 1])}
[Episode=190 Seed=18] Took 0.036293745040893555s to save gc logs
Finished after 1.0429546285337872 hrs
