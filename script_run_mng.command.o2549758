Mon Jul 10 10:36:32 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |
| 17%   20C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  On   | 00000000:1B:00.0 Off |                  N/A |
| 16%   25C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce ...  On   | 00000000:3D:00.0 Off |                  N/A |
| 16%   19C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce ...  On   | 00000000:3E:00.0 Off |                  N/A |
| 16%   20C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |
| 22%   42C    P2    67W / 250W |   9777MiB / 11178MiB |     14%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |
| 17%   20C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |
| 22%   42C    P2    70W / 250W |   9447MiB / 11178MiB |     12%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA GeForce ...  On   | 00000000:B2:00.0 Off |                  N/A |
| 16%   22C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    4   N/A  N/A    697444      C   python3                          9775MiB |
|    6   N/A  N/A    703857      C   python3                          9445MiB |
+-----------------------------------------------------------------------------+
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs/MiniGrid-MultiRoom-N4-S5-v0 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/logs/MiniGrid-MultiRoom-N4-S5-v0/18 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0/18 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0/18/initiation_set_plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0/18/value_function_plots 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0/18/accepted_events 
Successfully created the directory /ifs/CS/replicated/home/npermpre/hrl-with-dsg/plots/MiniGrid-MultiRoom-N4-S5-v0/18/rejected_events 
Creating classifier of type cnn
Creating rainbow with sigma=0.5
Created model-free option global-option with option_idx=0
[DSGTrainer] Adding new SalientEvent  SE([ 4 18])
[Seed=18] Device count: 1 Device Name: NVIDIA GeForce GTX 1080 Ti
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 0 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: 32.24438710231334
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 1 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.68488023430109
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 2 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -22.037777710473165
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 3 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -23.069377502659336
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 4 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -23.73651878896635
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 5 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -24.55134931035775
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 6 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -24.548186562606134
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 7 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -28.38450855296105
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 8 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -28.77176057640463
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 9 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -29.88993743591709
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 10 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -29.704754501290154
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 11 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -27.896683728438802
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 12 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -27.663995125563815
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 13 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -31.518884912133217
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 14 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -27.71533651810023
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 15 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.18835793621838
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 16 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -31.732683559974248
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 17 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.564766922965646
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 18 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.796647624112666
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 19 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.235854986240156
Accepted New Salient Event:  SE([10 15])
[DSGTrainer] Adding new SalientEvent  SE([10 15])
Took 3.877819538116455s to update distance table with 20040 states and events {SE([ 4 18]), None, SE([10 15])}
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 20 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -29.86775530705927
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 21 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.51581992348656
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 22 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.097193311259616
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 23 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.42079219222069
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 24 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.54109487426467
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 25 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -36.701380107784644
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 26 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -37.79927257215604
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 27 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -37.9329560212791
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 28 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.87240453995764
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 29 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.428463047370315
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 30 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.820697732269764
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 31 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.28111486695707
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 32 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.70906563248718
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 33 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -31.306031367217656
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 34 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -27.908886425720993
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 35 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -33.288227941753576
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 36 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.89423210080713
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 37 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -37.68618071638048
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 38 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.27076016133651
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 39 Step: 0
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.8172273254022
Accepted New Salient Event:  SE([10 16])
[DSGTrainer] Adding new SalientEvent  SE([10 16])
Took 4.279402256011963s to update distance table with 20040 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.19097547965066
Took 0.028565645217895508s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -22.31240238155806
Took 0.04474449157714844s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None, SE([10 15])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.11173876671819
Took 0.028348445892333984s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.84001311659813
Took 0.029191017150878906s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.953236060682684
Took 0.02761387825012207s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.79591692332178
Took 0.02791571617126465s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -37.24141462147236
Took 0.026053190231323242s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.281336914980784
Took 0.033780813217163086s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -36.35958127261256
Took 0.028467655181884766s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.57184033334488
Took 0.028889894485473633s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.5753866173327
Took 0.028530359268188477s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.911649361252785
Took 0.025680065155029297s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.77933848975226
Took 0.0298004150390625s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -42.64501065015793
Took 0.029898405075073242s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.20765842474066
Took 0.026381254196166992s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.6649710200727
Took 0.025354862213134766s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.79030868364498
Took 0.03232288360595703s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.53874057671055
Took 0.032975196838378906s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.5983622475178
Took 0.02657032012939453s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -42.40829785447568
Took 0.033360958099365234s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -18.023512572734035
Took 0.03733062744140625s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None, SE([10 15])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -36.30965849430504
Took 0.030974149703979492s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.4396175621514
Took 0.027979373931884766s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.44474821630865
Took 0.02697610855102539s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.70461924211122
Took 0.029130935668945312s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.816895660078444
Took 0.03084111213684082s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.064100672490895
Took 0.035504817962646484s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -41.32332715578377
Took 0.025476932525634766s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -41.22319374560175
Took 0.02857375144958496s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.644357555313036
Took 0.02679133415222168s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.58149711541864
Took 0.032257080078125s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None, SE([10 15])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.54909290606156
Took 0.031037569046020508s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -26.02870679051739
Took 0.027831315994262695s to update distance table with 1001 states and events {None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.651256291544996
Took 0.029767751693725586s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.9322747644037
Took 0.03461265563964844s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -34.27377702816739
Took 0.026960372924804688s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None, SE([10 15])}
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.52289720252156
Took 0.051955461502075195s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.12411502792747
Took 0.036138057708740234s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.91530161995615
Took 0.029572010040283203s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.82330185873434
Took 0.025422334671020508s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.19217145629227
Took 0.03790712356567383s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -41.99679442308843
Took 0.027792930603027344s to update distance table with 1001 states and events {SE([10 16]), SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.22864592121914
Took 0.035230159759521484s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -35.094964958036144
Took 0.026207447052001953s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.121177205001004
Took 0.037151336669921875s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.30603502690792
Took 0.018626689910888672s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.4350257711485
Took 0.025154829025268555s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -46.124441703315824
Took 0.04105663299560547s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.67876818496734
Took 0.03067612648010254s to update distance table with 1001 states and events {SE([ 4 18]), None}
[RND Rollout] Reward: 0.0	IntrinsicReward: -46.365966144017875
Took 0.03732562065124512s to update distance table with 1001 states and events {SE([ 4 18]), None}
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 90 Step: 0
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[DeepSkillGraphsAgent] Creating chain from SE([ 4 18]) -> SE([10 16])
Creating classifier of type cnn
Created model-free option goal-option-1 with option_idx=1
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002961874008178711s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.76837158203125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 91 Step: 120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002162933349609375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 92 Step: 240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002969980239868164s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 93 Step: 360
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[DeepSkillGraphsAgent] Creating chain from SE([ 4 18]) -> SE([10 15])
Creating classifier of type cnn
Created model-free option goal-option-2 with option_idx=2
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.003061056137084961s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 94 Step: 480
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0022749900817871094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 95 Step: 600
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0028219223022460938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 96 Step: 720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0035271644592285156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 97 Step: 840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0028352737426757812s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 98 Step: 960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0028107166290283203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 99 Step: 1080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029680728912353516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 100 Step: 1200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003312826156616211s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.0067901611328125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 101 Step: 1320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0031032562255859375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 102 Step: 1440
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0030028820037841797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 103 Step: 1560
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002788543701171875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 104 Step: 1680
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0030362606048583984s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 105 Step: 1800
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002474069595336914s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 106 Step: 1920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029342174530029297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 107 Step: 2040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029397010803222656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 108 Step: 2160
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0022292137145996094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 109 Step: 2280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029020309448242188s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 110 Step: 2400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002974271774291992s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.0067901611328125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 111 Step: 2520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0022020339965820312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 112 Step: 2640
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029022693634033203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 113 Step: 2760
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002904176712036133s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 114 Step: 2880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003073453903198242s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 115 Step: 3000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0036039352416992188s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 116 Step: 3120
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002960205078125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 117 Step: 3240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0028977394104003906s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 118 Step: 3360
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0028443336486816406s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 119 Step: 3480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029528141021728516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 120 Step: 3600
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0028603076934814453s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.76837158203125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 121 Step: 3720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003009796142578125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 122 Step: 3840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029785633087158203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 123 Step: 3960
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0030221939086914062s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 124 Step: 4080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029311180114746094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 125 Step: 4200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002946138381958008s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 126 Step: 4320
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029497146606445312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 127 Step: 4440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029239654541015625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 128 Step: 4560
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0032618045806884766s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 129 Step: 4680
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0034508705139160156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 130 Step: 4800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0033257007598876953s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.76837158203125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 131 Step: 4920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0032682418823242188s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 132 Step: 5040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0033583641052246094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 133 Step: 5160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029189586639404297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 134 Step: 5280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0032536983489990234s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 135 Step: 5400
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003430604934692383s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 136 Step: 5520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003383636474609375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 137 Step: 5640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0031769275665283203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 138 Step: 5760
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0032699108123779297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 139 Step: 5880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0035552978515625s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=140 Seed=18] Took 0.05522322654724121s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 140 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.462727033067495
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 141 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -42.91521046706475
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 142 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.05714778552647
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 143 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.08289906219579
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 144 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.21911549754441
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 145 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.94671299791662
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 146 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -38.895803071558475
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 147 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.58124022028642
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 148 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.1563408470829
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 149 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.88912860956043
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 150 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.768463503569365
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 151 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.67086638882756
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 152 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.98267505900003
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 153 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.98933614976704
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 154 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.76683370117098
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 155 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.629411708563566
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 156 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.25184680894017
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 157 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -46.409259243286215
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 158 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.72284411918372
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 159 Step: 6000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.29139907768695
Took 4.892225742340088s to update distance table with 20040 states and events {SE([10 16]), SE([ 4 18]), None}
[Episode=160 Seed=18] Took 0.06040835380554199s to save gc logs
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 160 Step: 6000
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.002880096435546875s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.245208740234375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 161 Step: 6120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0035915374755859375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 162 Step: 6240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003137826919555664s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 163 Step: 6360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0023887157440185547s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 164 Step: 6480
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0033309459686279297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 165 Step: 6600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0032854080200195312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 166 Step: 6720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0025701522827148438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 167 Step: 6840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029501914978027344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 168 Step: 6960
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.003213167190551758s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 169 Step: 7080
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002245187759399414s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 170 Step: 7200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-1 on 120 transitions
Took 0.0029315948486328125s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.4836273193359375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 171 Step: 7320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002929210662841797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 172 Step: 7440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
goal-option-1 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
2.3685859291019313
Updated goal-option-1 on 88 transitions
Took 0.002465486526489258s to update distance table with 89 states and events {SE([10 16]), SE([ 4 18]), None}
DSG successfully reached SE([10 16])
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[DeepSkillGraphsAgent] Creating chain from SE([10 16]) -> SE([10 15])
Creating classifier of type cnn
Created model-free option goal-option-3 with option_idx=3
[Graph Consolidation] From (10, 16) to [10 15]
Revised planner goal vertex to SE([10 16]) and dsc goal vertex to SE([10 15])
Planner goal: SE([10 16]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [10 16] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 32 transitions
Took 0.0008442401885986328s to update distance table with 33 states and events {SE([10 16]), None}
================================================================================
[Consolidation] Episode: 173 Step: 7560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0031533241271972656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 174 Step: 7680
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002360105514526367s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 175 Step: 7800
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0033807754516601562s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 176 Step: 7920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.710856251838246
Updated goal-option-1 on 120 transitions
Took 0.0029730796813964844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 177 Step: 8040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8193356915005188
Updated goal-option-1 on 120 transitions
Took 0.0031652450561523438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 178 Step: 8160
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0030214786529541016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 179 Step: 8280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.8533890533221036
Updated goal-option-1 on 120 transitions
Took 0.0032050609588623047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 180 Step: 8400
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0021953582763671875s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.291534423828125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 181 Step: 8520
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.0071972642089455
Updated goal-option-1 on 120 transitions
Took 0.0031180381774902344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 182 Step: 8640
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002990245819091797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 183 Step: 8760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002465486526489258s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 184 Step: 8880
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.003326416015625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 185 Step: 9000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.509337482390138
Updated goal-option-1 on 120 transitions
Took 0.0038080215454101562s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 186 Step: 9120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002526998519897461s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 187 Step: 9240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.28515514969912
Updated goal-option-1 on 120 transitions
Took 0.0032188892364501953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 188 Step: 9360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3073627916420933
Updated goal-option-1 on 120 transitions
Took 0.0032160282135009766s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 189 Step: 9480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002927064895629883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 190 Step: 9600
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.2006629673651443
Updated goal-option-1 on 120 transitions
Took 0.0034530162811279297s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 6.67572021484375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 191 Step: 9720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.7925831785953479
Updated goal-option-1 on 120 transitions
Took 0.0034661293029785156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 192 Step: 9840
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8655917716441213
Updated goal-option-1 on 120 transitions
Took 0.0034704208374023438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 193 Step: 9960
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.8662434317804333
Updated goal-option-1 on 120 transitions
Took 0.003944873809814453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 194 Step: 10080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.7347098429160255
Updated goal-option-1 on 120 transitions
Took 0.0032334327697753906s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 195 Step: 10200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029625892639160156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 196 Step: 10320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.7945378364886204
Updated goal-option-1 on 120 transitions
Took 0.003159761428833008s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 197 Step: 10440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.823681759959704
Updated goal-option-1 on 120 transitions
Took 0.003122568130493164s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 198 Step: 10560
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.300123062212628
Updated goal-option-1 on 120 transitions
Took 0.003251791000366211s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 199 Step: 10680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.13818381591022
Updated goal-option-1 on 120 transitions
Took 0.005605220794677734s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 200 Step: 10800
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.5513999824964904
Updated goal-option-1 on 120 transitions
Took 0.0029611587524414062s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 8.106231689453125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 201 Step: 10920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.651436507092842
Updated goal-option-1 on 120 transitions
Took 0.0030145645141601562s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 202 Step: 11040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0032389163970947266s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 203 Step: 11160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0033576488494873047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 204 Step: 11280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.9927757259245067
Updated goal-option-1 on 120 transitions
Took 0.003507852554321289s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 205 Step: 11400
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0028944015502929688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 206 Step: 11520
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6790670350469405
Updated goal-option-1 on 120 transitions
Took 0.003027200698852539s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 207 Step: 11640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029311180114746094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 208 Step: 11760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.592632366496694
Updated goal-option-1 on 120 transitions
Took 0.0037560462951660156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 209 Step: 11880
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4430185505649546
Updated goal-option-1 on 120 transitions
Took 0.003587961196899414s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=210 Seed=18] Took 0.06326580047607422s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 210 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -45.46892252238467
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 211 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.28774456772953
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 212 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.77270788571332
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 213 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.02674835367361
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 214 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.67630844004452
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 215 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.460192372091115
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 216 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.94036209688056
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 217 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.8495426196605
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 218 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.38274235196877
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 219 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.28667134261923
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 220 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.39330935571343
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 221 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.91465677879751
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 222 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.61374034918845
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 223 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.52374636428431
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 224 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.92609441280365
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 225 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.422878528013825
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 226 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.647817336022854
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 227 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.63070876524944
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 228 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.1560978917405
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 229 Step: 12000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.8843608146999
Took 5.463900089263916s to update distance table with 20040 states and events {SE([ 4 18]), None}
[Episode=230 Seed=18] Took 0.08080554008483887s to save gc logs
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 230 Step: 12000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029137134552001953s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.76837158203125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 231 Step: 12120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002999544143676758s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 232 Step: 12240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029735565185546875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 233 Step: 12360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9094885714617299
Updated goal-option-1 on 120 transitions
Took 0.002950429916381836s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 234 Step: 12480
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4900066518230568
Updated goal-option-1 on 120 transitions
Took 0.0031280517578125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 235 Step: 12600
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0843760147938828
Updated goal-option-1 on 120 transitions
Took 0.0033478736877441406s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 236 Step: 12720
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0024733543395996094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 237 Step: 12840
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6272268060281145
Updated goal-option-1 on 120 transitions
Took 0.0029675960540771484s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 238 Step: 12960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029935836791992188s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 239 Step: 13080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.970738756896336
Updated goal-option-1 on 120 transitions
Took 0.003821849822998047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 240 Step: 13200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002965688705444336s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.76837158203125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 241 Step: 13320
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.003175020217895508s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 242 Step: 13440
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.7354756604660109
Updated goal-option-1 on 120 transitions
Took 0.003114938735961914s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 243 Step: 13560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0029518604278564453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 244 Step: 13680
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0031080245971679688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 245 Step: 13800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.935050195694567
Updated goal-option-1 on 120 transitions
Took 0.003262758255004883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 246 Step: 13920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.7500178233055137
Updated goal-option-1 on 120 transitions
Took 0.003417491912841797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 247 Step: 14040
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.003313302993774414s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 248 Step: 14160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.2225995569076047
Updated goal-option-1 on 120 transitions
Took 0.0031976699829101562s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 249 Step: 14280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.3062445525397623
Updated goal-option-1 on 120 transitions
Took 0.0030281543731689453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 250 Step: 14400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002963542938232422s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.5299530029296875e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 251 Step: 14520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.9718741650171445
Updated goal-option-1 on 120 transitions
Took 0.003047943115234375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 252 Step: 14640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.7946638974185127
Updated goal-option-1 on 120 transitions
Took 0.003354310989379883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 253 Step: 14760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.0173028206436805
Updated goal-option-1 on 120 transitions
Took 0.003286123275756836s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 254 Step: 14880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.002939939498901367s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 255 Step: 15000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.9100511416755008
Updated goal-option-1 on 120 transitions
Took 0.002985239028930664s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 256 Step: 15120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.247694940697672
Updated goal-option-1 on 120 transitions
Took 0.0031108856201171875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 257 Step: 15240
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.221968464548102
Updated goal-option-1 on 120 transitions
Took 0.0031480789184570312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 258 Step: 15360
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.6215706775734784
Updated goal-option-1 on 120 transitions
Took 0.002962350845336914s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 259 Step: 15480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-2 on 120 transitions
Took 0.0022001266479492188s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 260 Step: 15600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3782284250469958
Updated goal-option-1 on 120 transitions
Took 0.0031447410583496094s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 261 Step: 15720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.38346481 0.61653519]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
goal-option-1 reached term set 2 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.6985787578629532
Updated goal-option-1 on 66 transitions
Took 0.0016696453094482422s to update distance table with 67 states and events {SE([10 16]), SE([ 4 18]), None}
DSG successfully reached SE([10 16])
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (10, 16) to [10 15]
Revised planner goal vertex to SE([10 16]) and dsc goal vertex to SE([10 15])
Planner goal: SE([10 16]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [10 16] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
goal-option-2 reached term set 1 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
5.467754420890873
Updated goal-option-2 on 6 transitions
Took 0.00024700164794921875s to update distance table with 7 states and events {SE([10 16]), SE([10 15])}
DSG successfully reached SE([10 15])
[Random] Deep skill graphs target event: SE([ 4 18])
[Random] DSG selected event SE([ 4 18])
[DeepSkillGraphsAgent] Creating chain from SE([10 15]) -> SE([ 4 18])
Creating classifier of type cnn
Created model-free option goal-option-4 with option_idx=4
[Graph Consolidation] From (10, 15) to [ 4 18]
Revised planner goal vertex to SE([10 15]) and dsc goal vertex to SE([ 4 18])
Planner goal: SE([10 15]), DSC goal: SE([ 4 18]) and Goal: SE([ 4 18])
False
False
Rolling out DSC with goal vertex SE([ 4 18])
Rolling out goal-option-4, from [10 15] targeting {'player_x': 4, 'player_y': 18, 'inventory': '00000000', 'room_number': 1, 'dead': False, 'falling': False, 'left_door_open': False, 'right_door_open': False}
[-trajReplay] Replaying negative trajectory with positive relabel
Updated goal-option-4 on 48 transitions
Took 0.0013666152954101562s to update distance table with 49 states and events {None, SE([10 15])}
================================================================================
[Consolidation] Episode: 262 Step: 15840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.935311981359872
Updated goal-option-1 on 120 transitions
Took 0.003281831741333008s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 263 Step: 15960
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.4584402264024714
Updated goal-option-1 on 120 transitions
Took 0.003306865692138672s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 264 Step: 16080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.9649298011547043
Updated goal-option-1 on 120 transitions
Took 0.0031735897064208984s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 265 Step: 16200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.905114275358972
Updated goal-option-2 on 120 transitions
Took 0.003181934356689453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 266 Step: 16320
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.5071657750639549
Updated goal-option-1 on 120 transitions
Took 0.0033409595489501953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 267 Step: 16440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4770349826637679
Updated goal-option-1 on 120 transitions
Took 0.0032656192779541016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 268 Step: 16560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.5741758276541782
Updated goal-option-1 on 120 transitions
Took 0.0029990673065185547s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 269 Step: 16680
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.197122094610957
Updated goal-option-1 on 120 transitions
Took 0.0029480457305908203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 270 Step: 16800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.059471350359108
Updated goal-option-1 on 120 transitions
Took 0.0031728744506835938s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 2.1219253540039062e-05s to add potential edges.
================================================================================
[Consolidation] Episode: 271 Step: 16920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.502479407454141
Updated goal-option-1 on 120 transitions
Took 0.003103971481323242s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 272 Step: 17040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1828110164549661
Updated goal-option-1 on 120 transitions
Took 0.003132343292236328s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 273 Step: 17160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
4.858097725021209
Updated goal-option-2 on 120 transitions
Took 0.003116607666015625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 274 Step: 17280
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.593967929676708
Updated goal-option-1 on 120 transitions
Took 0.0031332969665527344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 275 Step: 17400
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.9564769390097223
Updated goal-option-2 on 120 transitions
Took 0.003045797348022461s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 276 Step: 17520
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6093111765220378
Updated goal-option-1 on 120 transitions
Took 0.0035202503204345703s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 277 Step: 17640
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.167035899510362
Updated goal-option-2 on 120 transitions
Took 0.0028641223907470703s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 278 Step: 17760
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3802878258796045
Updated goal-option-1 on 120 transitions
Took 0.003987312316894531s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 279 Step: 17880
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.18124107667245
Updated goal-option-2 on 120 transitions
Took 0.002902507781982422s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=280 Seed=18] Took 0.009044170379638672s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 280 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.80832965299487
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 281 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.97332521213684
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 282 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -46.307575807964895
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 283 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -43.665353829739615
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 284 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.9217215757817
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 285 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.673446564003825
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 286 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.68502392247319
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 287 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.0773592448968
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 288 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.765172329120105
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 289 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.26157179661095
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 290 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.18648604070768
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 291 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.40526322182268
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 292 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -41.38504090045171
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 293 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.67191086837556
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 294 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.791669661179185
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 295 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.823606772813946
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 296 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.30808843160048
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 297 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.91092336177826
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 298 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -44.23424600198632
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 299 Step: 18000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.82194123300724
Took 6.4585864543914795s to update distance table with 20040 states and events {SE([ 4 18]), None}
[Episode=300 Seed=18] Took 0.0676720142364502s to save gc logs
Enter Visualization
[]
[SE([10 15]), SE([10 16])]
PLOTS
[]
[]
PLOTS2
[10, 10]
[1, 1]
END UTILS
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 300 Step: 18000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3584826714553266
Updated goal-option-1 on 120 transitions
Took 0.0035715103149414062s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 6.4373016357421875e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 301 Step: 18120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6813825313012762
Updated goal-option-1 on 120 transitions
Took 0.0029859542846679688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 302 Step: 18240
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
goal-option-1 reached term set 3 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.566402856383566
Updated goal-option-1 on 93 transitions
Took 0.0027565956115722656s to update distance table with 94 states and events {SE([10 16]), SE([ 4 18]), None}
DSG successfully reached SE([10 16])
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (10, 16) to [10 15]
Revised planner goal vertex to SE([10 16]) and dsc goal vertex to SE([10 15])
Planner goal: SE([10 16]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [10 16] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
11.641989168134472
Updated goal-option-2 on 27 transitions
Took 0.0007598400115966797s to update distance table with 28 states and events {SE([10 16]), None}
================================================================================
[Consolidation] Episode: 303 Step: 18360
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.068149106128008
Updated goal-option-2 on 120 transitions
Took 0.002918720245361328s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 304 Step: 18480
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.360067655822751
Updated goal-option-1 on 120 transitions
Took 0.00426483154296875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 305 Step: 18600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9875545255574321
Updated goal-option-1 on 120 transitions
Took 0.003181934356689453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 306 Step: 18720
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
14.147049898074734
Updated goal-option-2 on 120 transitions
Took 0.002851247787475586s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 307 Step: 18840
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.6268751912358963
Updated goal-option-1 on 120 transitions
Took 0.0036144256591796875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 308 Step: 18960
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4481508561022378
Updated goal-option-1 on 120 transitions
Took 0.0037004947662353516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 309 Step: 19080
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2306403247390738
Updated goal-option-1 on 120 transitions
Took 0.0034415721893310547s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 310 Step: 19200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.902503662742674
Updated goal-option-2 on 120 transitions
Took 0.0029969215393066406s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 6.67572021484375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 311 Step: 19320
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3897716838523688
Updated goal-option-1 on 120 transitions
Took 0.003323793411254883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 312 Step: 19440
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3611720380388348
Updated goal-option-1 on 120 transitions
Took 0.0034499168395996094s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 313 Step: 19560
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
14.757118907712753
Updated goal-option-2 on 120 transitions
Took 0.003040790557861328s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 314 Step: 19680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4421814180581531
Updated goal-option-1 on 120 transitions
Took 0.0034639835357666016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 315 Step: 19800
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2290137014634548
Updated goal-option-1 on 120 transitions
Took 0.0032606124877929688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 316 Step: 19920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2310795541300479
Updated goal-option-1 on 120 transitions
Took 0.0033102035522460938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 317 Step: 20040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9817843782597414
Updated goal-option-1 on 120 transitions
Took 0.0033311843872070312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 318 Step: 20160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1605661677118353
Updated goal-option-1 on 120 transitions
Took 0.004465579986572266s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 319 Step: 20280
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
4.638244028668851
Updated goal-option-2 on 120 transitions
Took 0.0028579235076904297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 320 Step: 20400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2821614111895092
Updated goal-option-1 on 120 transitions
Took 0.003570556640625s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.9604644775390625e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 321 Step: 20520
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.86371338606945
Updated goal-option-2 on 120 transitions
Took 0.0028603076934814453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 322 Step: 20640
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.201747813505962
Updated goal-option-2 on 120 transitions
Took 0.002989530563354492s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 323 Step: 20760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.257022333462721
Updated goal-option-1 on 120 transitions
Took 0.0033233165740966797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 324 Step: 20880
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2655488873061576
Updated goal-option-1 on 120 transitions
Took 0.0035452842712402344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 325 Step: 21000
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.869187815735737
Updated goal-option-2 on 120 transitions
Took 0.0029554367065429688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 326 Step: 21120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.9502292112558197
Updated goal-option-2 on 120 transitions
Took 0.0029549598693847656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 327 Step: 21240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0875281679146716
Updated goal-option-1 on 120 transitions
Took 0.0035114288330078125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 328 Step: 21360
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.403811628797225
Updated goal-option-2 on 120 transitions
Took 0.0033321380615234375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 329 Step: 21480
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.10478959521944
Updated goal-option-2 on 120 transitions
Took 0.0030317306518554688s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 330 Step: 21600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
2.225189259349247
Updated goal-option-2 on 120 transitions
Took 0.0028769969940185547s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 331 Step: 21720
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1219906816451186
Updated goal-option-1 on 120 transitions
Took 0.003172636032104492s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 332 Step: 21840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4672517885863614
Updated goal-option-1 on 120 transitions
Took 0.0032334327697753906s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 333 Step: 21960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.554336325142642
Updated goal-option-2 on 120 transitions
Took 0.0033752918243408203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 334 Step: 22080
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
goal-option-1 reached term set 4 times.
[+SalientEventTrajReplay] Replaying positive trajectory with *positive* original goal
1.3428289115980732
Updated goal-option-1 on 108 transitions
Took 0.0031647682189941406s to update distance table with 109 states and events {SE([10 16]), SE([ 4 18]), None}
DSG successfully reached SE([10 16])
Unconnected Events: [SE([ 4 18]), SE([10 15])] | Probs: [[2.08598118e-21 1.00000000e+00]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (10, 16) to [10 15]
Revised planner goal vertex to SE([10 16]) and dsc goal vertex to SE([10 15])
Planner goal: SE([10 16]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [10 16] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.665559174049469
Updated goal-option-2 on 12 transitions
Took 0.0004944801330566406s to update distance table with 13 states and events {SE([10 16])}
================================================================================
[Consolidation] Episode: 335 Step: 22200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.757160314137026
Updated goal-option-2 on 120 transitions
Took 0.0032472610473632812s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 336 Step: 22320
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.395146970947583
Updated goal-option-2 on 120 transitions
Took 0.003245115280151367s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 337 Step: 22440
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.039091780080828
Updated goal-option-2 on 120 transitions
Took 0.002918720245361328s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 338 Step: 22560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.5607966869359924
Updated goal-option-2 on 120 transitions
Took 0.0031495094299316406s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 339 Step: 22680
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0657663952125689
Updated goal-option-1 on 120 transitions
Took 0.0036001205444335938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 340 Step: 22800
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.798723860998595
Updated goal-option-2 on 120 transitions
Took 0.0029306411743164062s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 341 Step: 22920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.043027767452409
Updated goal-option-1 on 120 transitions
Took 0.003942251205444336s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 342 Step: 23040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.415462677190261
Updated goal-option-1 on 120 transitions
Took 0.003474712371826172s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 343 Step: 23160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3370916844429308
Updated goal-option-1 on 120 transitions
Took 0.003582477569580078s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 344 Step: 23280
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
11.399882879551678
Updated goal-option-2 on 120 transitions
Took 0.0029871463775634766s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 345 Step: 23400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.947049818389381
Updated goal-option-1 on 120 transitions
Took 0.0036106109619140625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 346 Step: 23520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.147095750782236
Updated goal-option-1 on 120 transitions
Took 0.003231048583984375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 347 Step: 23640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.269777607460994
Updated goal-option-2 on 120 transitions
Took 0.002895355224609375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 348 Step: 23760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0834142710566521
Updated goal-option-1 on 120 transitions
Took 0.0034034252166748047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 349 Step: 23880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7739237241579938
Updated goal-option-1 on 120 transitions
Took 0.0036661624908447266s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=350 Seed=18] Took 0.012984752655029297s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 350 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.42504625953734
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 351 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.14164797589183
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 352 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.84077308035921
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 353 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.735654019750655
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 354 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.84946107817814
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 355 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.843471786007285
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 356 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.71595651470125
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 357 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.880396477121394
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 358 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.48080015426967
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 359 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.69473802018911
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 360 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.92846561665647
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 361 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.82207146828296
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 362 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.636381399817765
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 363 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.41468862904003
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 364 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.19858372025192
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 365 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.56281438842416
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 366 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.212964047444984
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 367 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.27968816680368
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 368 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -39.823814495350234
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 369 Step: 24000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.93490062467754
Took 5.903761625289917s to update distance table with 20040 states and events {SE([ 4 18]), None}
[Episode=370 Seed=18] Took 0.11821794509887695s to save gc logs
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 370 Step: 24000
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.14022722299255
Updated goal-option-2 on 120 transitions
Took 0.0033931732177734375s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 371 Step: 24120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
10.154401393252469
Updated goal-option-2 on 120 transitions
Took 0.003345489501953125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 372 Step: 24240
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.5556963993464406
Updated goal-option-2 on 120 transitions
Took 0.003231525421142578s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 373 Step: 24360
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.040876353026458
Updated goal-option-1 on 120 transitions
Took 0.003064393997192383s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 374 Step: 24480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.324302913465848
Updated goal-option-2 on 120 transitions
Took 0.0033593177795410156s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 375 Step: 24600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0855183542845925
Updated goal-option-1 on 120 transitions
Took 0.002896547317504883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 376 Step: 24720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.220467782867478
Updated goal-option-1 on 120 transitions
Took 0.003003835678100586s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 377 Step: 24840
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.607039305071036
Updated goal-option-2 on 120 transitions
Took 0.003164529800415039s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 378 Step: 24960
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4671524796500426
Updated goal-option-1 on 120 transitions
Took 0.0034482479095458984s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 379 Step: 25080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9503232880724674
Updated goal-option-1 on 120 transitions
Took 0.0029637813568115234s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 380 Step: 25200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.4967862244013281
Updated goal-option-1 on 120 transitions
Took 0.0033512115478515625s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 6.67572021484375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 381 Step: 25320
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9007649804266403
Updated goal-option-1 on 120 transitions
Took 0.0035195350646972656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 382 Step: 25440
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.057059606366871
Updated goal-option-1 on 120 transitions
Took 0.0035300254821777344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 383 Step: 25560
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
4.834964541531008
Updated goal-option-2 on 120 transitions
Took 0.0029036998748779297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 384 Step: 25680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0424843293609705
Updated goal-option-1 on 120 transitions
Took 0.0034422874450683594s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 385 Step: 25800
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.724856904475018
Updated goal-option-2 on 120 transitions
Took 0.002839326858520508s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 386 Step: 25920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9163450221578809
Updated goal-option-1 on 120 transitions
Took 0.003473520278930664s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 387 Step: 26040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.083133427458034
Updated goal-option-1 on 120 transitions
Took 0.0034868717193603516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 388 Step: 26160
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.950548029194275
Updated goal-option-2 on 120 transitions
Took 0.0030024051666259766s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 389 Step: 26280
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.9563053175807
Updated goal-option-2 on 120 transitions
Took 0.002889394760131836s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 390 Step: 26400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7407240338726265
Updated goal-option-1 on 120 transitions
Took 0.0037093162536621094s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 6.9141387939453125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 391 Step: 26520
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.681667801613609
Updated goal-option-2 on 120 transitions
Took 0.0030870437622070312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 392 Step: 26640
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.761581857999166
Updated goal-option-2 on 120 transitions
Took 0.002943277359008789s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 393 Step: 26760
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.136091345133969
Updated goal-option-2 on 120 transitions
Took 0.00313568115234375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 394 Step: 26880
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.3990934542780895
Updated goal-option-1 on 120 transitions
Took 0.003385782241821289s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 395 Step: 27000
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1530590194115924
Updated goal-option-1 on 120 transitions
Took 0.003226041793823242s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 396 Step: 27120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1028512767630752
Updated goal-option-1 on 120 transitions
Took 0.003312826156616211s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 397 Step: 27240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.956416896253359
Updated goal-option-2 on 120 transitions
Took 0.0029382705688476562s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 398 Step: 27360
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.211999378477534
Updated goal-option-2 on 120 transitions
Took 0.002960205078125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 399 Step: 27480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
10.637429669427332
Updated goal-option-2 on 120 transitions
Took 0.002902507781982422s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 400 Step: 27600
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8757436287708131
Updated goal-option-1 on 120 transitions
Took 0.003458261489868164s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 2.3126602172851562e-05s to add potential edges.
================================================================================
[Consolidation] Episode: 401 Step: 27720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.134379377575147
Updated goal-option-1 on 120 transitions
Took 0.0031757354736328125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 402 Step: 27840
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.087795303072259
Updated goal-option-1 on 120 transitions
Took 0.0034482479095458984s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 403 Step: 27960
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9789354985990882
Updated goal-option-1 on 120 transitions
Took 0.0031592845916748047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 404 Step: 28080
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.047751062549651
Updated goal-option-2 on 120 transitions
Took 0.003050565719604492s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 405 Step: 28200
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.951957924380189
Updated goal-option-2 on 120 transitions
Took 0.002938508987426758s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 406 Step: 28320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.855213121976703
Updated goal-option-2 on 120 transitions
Took 0.003029346466064453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 407 Step: 28440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7633705277342114
Updated goal-option-1 on 120 transitions
Took 0.0031642913818359375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 408 Step: 28560
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.500697416696875
Updated goal-option-2 on 120 transitions
Took 0.003446817398071289s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 409 Step: 28680
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
4.272228282565872
Updated goal-option-2 on 120 transitions
Took 0.0029702186584472656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 410 Step: 28800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.165023236024665
Updated goal-option-1 on 120 transitions
Took 0.0031900405883789062s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.4836273193359375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 411 Step: 28920
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9409525497428914
Updated goal-option-1 on 120 transitions
Took 0.003134489059448242s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 412 Step: 29040
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.018997228890102
Updated goal-option-2 on 120 transitions
Took 0.0029714107513427734s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 413 Step: 29160
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.048905203656648
Updated goal-option-2 on 120 transitions
Took 0.002989053726196289s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 414 Step: 29280
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9560874173797562
Updated goal-option-1 on 120 transitions
Took 0.0034325122833251953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 415 Step: 29400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9691102243774234
Updated goal-option-1 on 120 transitions
Took 0.0033342838287353516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 416 Step: 29520
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
11.62371434768041
Updated goal-option-2 on 120 transitions
Took 0.002938985824584961s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 417 Step: 29640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
4.8805461686376175
Updated goal-option-2 on 120 transitions
Took 0.002841949462890625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 418 Step: 29760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0868519171276707
Updated goal-option-1 on 120 transitions
Took 0.0034999847412109375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 419 Step: 29880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.467244842400154
Updated goal-option-2 on 120 transitions
Took 0.002961874008178711s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=420 Seed=18] Took 0.0796515941619873s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 420 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.527747803134844
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 421 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -49.15104091865942
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 422 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.436162640340626
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 423 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.430661580991
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 424 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -40.641771293572674
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 425 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -54.751134369987994
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 426 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.085105647624005
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 427 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.45973488385789
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 428 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.83802364394069
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 429 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -53.77487226840276
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 430 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.82383317872882
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 431 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.93085558153689
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 432 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -47.66086128388997
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 433 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.793783489149064
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 434 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -57.76731779798865
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 435 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.88360273126091
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 436 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.19956302456558
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 437 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.29757891409099
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 438 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.1721770549193
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 439 Step: 30000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.72111100656912
Took 7.668782949447632s to update distance table with 20040 states and events {SE([10 16]), SE([ 4 18]), None}
[Episode=440 Seed=18] Took 0.06757831573486328s to save gc logs
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 440 Step: 30000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7922322419297189
Updated goal-option-1 on 120 transitions
Took 0.0032236576080322266s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 441 Step: 30120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.225194697268307
Updated goal-option-2 on 120 transitions
Took 0.0029392242431640625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 442 Step: 30240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2819680836316885
Updated goal-option-1 on 120 transitions
Took 0.0031790733337402344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 443 Step: 30360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.051664000709832
Updated goal-option-1 on 120 transitions
Took 0.003220796585083008s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 444 Step: 30480
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
13.496786497954632
Updated goal-option-2 on 120 transitions
Took 0.0029587745666503906s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 445 Step: 30600
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9427542595525957
Updated goal-option-1 on 120 transitions
Took 0.0033388137817382812s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 446 Step: 30720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
13.93774106300303
Updated goal-option-2 on 120 transitions
Took 0.0029883384704589844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 447 Step: 30840
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9876718313621446
Updated goal-option-1 on 120 transitions
Took 0.003779888153076172s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 448 Step: 30960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.949460941050867
Updated goal-option-2 on 120 transitions
Took 0.0029296875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 449 Step: 31080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7503985343972852
Updated goal-option-1 on 120 transitions
Took 0.0031981468200683594s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 450 Step: 31200
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.440098903010949
Updated goal-option-2 on 120 transitions
Took 0.003423452377319336s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 4.291534423828125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 451 Step: 31320
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9399380735271462
Updated goal-option-1 on 120 transitions
Took 0.0032765865325927734s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 452 Step: 31440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9574968212760048
Updated goal-option-1 on 120 transitions
Took 0.003171205520629883s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 453 Step: 31560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.128578517001999
Updated goal-option-1 on 120 transitions
Took 0.0032913684844970703s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 454 Step: 31680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0662831717997159
Updated goal-option-1 on 120 transitions
Took 0.0031881332397460938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 455 Step: 31800
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9258525108714993
Updated goal-option-1 on 120 transitions
Took 0.0032248497009277344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 456 Step: 31920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9636334734372142
Updated goal-option-1 on 120 transitions
Took 0.0033240318298339844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 457 Step: 32040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0099065721272082
Updated goal-option-1 on 120 transitions
Took 0.003462553024291992s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 458 Step: 32160
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.132536043723425
Updated goal-option-2 on 120 transitions
Took 0.0029981136322021484s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 459 Step: 32280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1396781870294785
Updated goal-option-1 on 120 transitions
Took 0.002973318099975586s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 460 Step: 32400
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.295065032229537
Updated goal-option-2 on 120 transitions
Took 0.0030164718627929688s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.0067901611328125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 461 Step: 32520
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.5356364224571735
Updated goal-option-2 on 120 transitions
Took 0.002911806106567383s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 462 Step: 32640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0773592219060468
Updated goal-option-1 on 120 transitions
Took 0.0032553672790527344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 463 Step: 32760
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.750526162213646
Updated goal-option-2 on 120 transitions
Took 0.0031173229217529297s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 464 Step: 32880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9202983371722757
Updated goal-option-1 on 120 transitions
Took 0.003726482391357422s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 465 Step: 33000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.079384684007186
Updated goal-option-1 on 120 transitions
Took 0.0032644271850585938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 466 Step: 33120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.572905209795054
Updated goal-option-2 on 120 transitions
Took 0.0035991668701171875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 467 Step: 33240
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.852000586921349
Updated goal-option-2 on 120 transitions
Took 0.002933979034423828s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 468 Step: 33360
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
13.22028641651074
Updated goal-option-2 on 120 transitions
Took 0.0029404163360595703s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 469 Step: 33480
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9159431083162698
Updated goal-option-1 on 120 transitions
Took 0.003556966781616211s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 470 Step: 33600
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.415851929535468
Updated goal-option-2 on 120 transitions
Took 0.0029633045196533203s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.245208740234375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 471 Step: 33720
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8858005510135071
Updated goal-option-1 on 120 transitions
Took 0.003165721893310547s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 472 Step: 33840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
16.556154378689826
Updated goal-option-2 on 120 transitions
Took 0.0029287338256835938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 473 Step: 33960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
10.304563589394093
Updated goal-option-2 on 120 transitions
Took 0.0028905868530273438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 474 Step: 34080
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.932835128138958
Updated goal-option-1 on 120 transitions
Took 0.0036437511444091797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 475 Step: 34200
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
8.982460035049977
Updated goal-option-2 on 120 transitions
Took 0.0032808780670166016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 476 Step: 34320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1836974943373044
Updated goal-option-1 on 120 transitions
Took 0.00380706787109375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 477 Step: 34440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9070538588004594
Updated goal-option-1 on 120 transitions
Took 0.0037457942962646484s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 478 Step: 34560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9370652932246256
Updated goal-option-1 on 120 transitions
Took 0.003211498260498047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 479 Step: 34680
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8270559554327321
Updated goal-option-1 on 120 transitions
Took 0.0031120777130126953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 480 Step: 34800
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.070825596617546
Updated goal-option-1 on 120 transitions
Took 0.0030050277709960938s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.245208740234375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 481 Step: 34920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0623179407238181
Updated goal-option-1 on 120 transitions
Took 0.0031266212463378906s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 482 Step: 35040
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2115899766155285
Updated goal-option-1 on 120 transitions
Took 0.003316164016723633s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 483 Step: 35160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.896551454351062
Updated goal-option-2 on 120 transitions
Took 0.0033881664276123047s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 484 Step: 35280
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1641812813651176
Updated goal-option-1 on 120 transitions
Took 0.0032134056091308594s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 485 Step: 35400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0258696867056296
Updated goal-option-1 on 120 transitions
Took 0.0031719207763671875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 486 Step: 35520
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.566774669098474
Updated goal-option-2 on 120 transitions
Took 0.0034155845642089844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 487 Step: 35640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2041856813597929
Updated goal-option-1 on 120 transitions
Took 0.0028905868530273438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 488 Step: 35760
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.699567890027538
Updated goal-option-2 on 120 transitions
Took 0.003016948699951172s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 489 Step: 35880
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0837111553282774
Updated goal-option-1 on 120 transitions
Took 0.003084421157836914s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=490 Seed=18] Took 0.008311748504638672s to save gc logs
Enter Visualization
[]
[SE([10 15]), SE([10 16])]
PLOTS
[]
[]
PLOTS2
[10, 10]
[1, 1]
END UTILS
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 490 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -51.29367381648626
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 491 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.450517115183175
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 492 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.04417831450701
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 493 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -55.2652984559536
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 494 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -57.189236868638545
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 495 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.746734108775854
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 496 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.04589512888924
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 497 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.97600834816694
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 498 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.92081523220986
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 499 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -58.150180604308844
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 500 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -48.89703121967614
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 501 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.52030155051034
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 502 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -57.63633312494494
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 503 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -58.302196899428964
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 504 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -50.50120439089369
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 505 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -57.66803403566155
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 506 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -58.02976671233773
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 507 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -58.71630287915468
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 508 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -57.36730559542775
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 509 Step: 36000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.7282814514474
Took 8.680856704711914s to update distance table with 20040 states and events {SE([ 4 18]), None}
[Episode=510 Seed=18] Took 0.07729840278625488s to save gc logs
[SE([ 4 18]), SE([10 15]), SE([10 16])]
================================================================================
[Consolidation] Episode: 510 Step: 36000
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7790836227621597
Updated goal-option-1 on 120 transitions
Took 0.003179788589477539s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 511 Step: 36120
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1055884807845067
Updated goal-option-1 on 120 transitions
Took 0.0030405521392822266s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 512 Step: 36240
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.91608269753245
Updated goal-option-2 on 120 transitions
Took 0.002963542938232422s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 513 Step: 36360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.263812728058112
Updated goal-option-1 on 120 transitions
Took 0.003239154815673828s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 514 Step: 36480
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.344471362803038
Updated goal-option-1 on 120 transitions
Took 0.003579854965209961s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 515 Step: 36600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1479273606211897
Updated goal-option-1 on 120 transitions
Took 0.0029659271240234375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 516 Step: 36720
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
9.716819737904839
Updated goal-option-2 on 120 transitions
Took 0.003101348876953125s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 517 Step: 36840
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.91809931366394
Updated goal-option-2 on 120 transitions
Took 0.002839803695678711s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 518 Step: 36960
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.555824847891927
Updated goal-option-2 on 120 transitions
Took 0.0032091140747070312s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 519 Step: 37080
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2012536084450207
Updated goal-option-1 on 120 transitions
Took 0.0031838417053222656s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 520 Step: 37200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.163247619529144
Updated goal-option-1 on 120 transitions
Took 0.0032320022583007812s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 521 Step: 37320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9211022377436149
Updated goal-option-1 on 120 transitions
Took 0.0031664371490478516s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 522 Step: 37440
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1283117248889247
Updated goal-option-1 on 120 transitions
Took 0.0031719207763671875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 523 Step: 37560
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.002665462571064
Updated goal-option-2 on 120 transitions
Took 0.002932310104370117s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 524 Step: 37680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0245451649966468
Updated goal-option-1 on 120 transitions
Took 0.0033164024353027344s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 525 Step: 37800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8746137051726315
Updated goal-option-1 on 120 transitions
Took 0.0032012462615966797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 526 Step: 37920
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0734438274627283
Updated goal-option-1 on 120 transitions
Took 0.0032417774200439453s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 527 Step: 38040
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2020282528495674
Updated goal-option-1 on 120 transitions
Took 0.0035860538482666016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 528 Step: 38160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.7909720495907503
Updated goal-option-1 on 120 transitions
Took 0.0032744407653808594s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 529 Step: 38280
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8413411878202034
Updated goal-option-1 on 120 transitions
Took 0.0031669139862060547s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 530 Step: 38400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.229495144386549
Updated goal-option-1 on 120 transitions
Took 0.0035331249237060547s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.7220458984375e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 531 Step: 38520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.059642011673539
Updated goal-option-1 on 120 transitions
Took 0.0034172534942626953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 532 Step: 38640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.2296172438266995
Updated goal-option-2 on 120 transitions
Took 0.0028896331787109375s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 533 Step: 38760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
5.4098364583527045
Updated goal-option-2 on 120 transitions
Took 0.003039836883544922s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 534 Step: 38880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0839253260701531
Updated goal-option-1 on 120 transitions
Took 0.0032596588134765625s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 535 Step: 39000
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2136946156235886
Updated goal-option-1 on 120 transitions
Took 0.0034399032592773438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 536 Step: 39120
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
3.8761768128412464
Updated goal-option-2 on 120 transitions
Took 0.002935647964477539s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 537 Step: 39240
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.119185714414816
Updated goal-option-1 on 120 transitions
Took 0.003543376922607422s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 538 Step: 39360
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9608487806270719
Updated goal-option-1 on 120 transitions
Took 0.0029726028442382812s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 539 Step: 39480
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.085284217286097
Updated goal-option-1 on 120 transitions
Took 0.0030486583709716797s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 540 Step: 39600
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0813840829064656
Updated goal-option-1 on 120 transitions
Took 0.00408935546875s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 7.62939453125e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 541 Step: 39720
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.160489287966197
Updated goal-option-1 on 120 transitions
Took 0.003595113754272461s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 542 Step: 39840
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
10.218720741531191
Updated goal-option-2 on 120 transitions
Took 0.0030345916748046875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 543 Step: 39960
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9716647483328367
Updated goal-option-1 on 120 transitions
Took 0.002991914749145508s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 544 Step: 40080
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0331486035193242
Updated goal-option-1 on 120 transitions
Took 0.0032491683959960938s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 545 Step: 40200
================================================================================
[Random] Deep skill graphs target event: SE([10 16])
[Random] DSG selected event SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.2425035201790942
Updated goal-option-1 on 120 transitions
Took 0.0037059783935546875s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 546 Step: 40320
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0748236202831376
Updated goal-option-1 on 120 transitions
Took 0.002864360809326172s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 547 Step: 40440
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.933842814944761
Updated goal-option-1 on 120 transitions
Took 0.0032324790954589844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 548 Step: 40560
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8893254031223812
Updated goal-option-1 on 120 transitions
Took 0.0029604434967041016s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 549 Step: 40680
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 790, 'bonus': 0.035578403348241, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 18333, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8484321793862996
Updated goal-option-1 on 120 transitions
Took 0.0032155513763427734s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 550 Step: 40800
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 781, 'bonus': 0.03578281334822566, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 7528, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9234458230278776
Updated goal-option-1 on 120 transitions
Took 0.0034644603729248047s to update distance table with 121 states and events {SE([ 4 18]), None}
Took 5.9604644775390625e-06s to add potential edges.
================================================================================
[Consolidation] Episode: 551 Step: 40920
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.876209577634221
Updated goal-option-2 on 120 transitions
Took 0.0030455589294433594s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 552 Step: 41040
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
6.533106335643727
Updated goal-option-2 on 120 transitions
Took 0.0029973983764648438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 553 Step: 41160
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.0917699545749904
Updated goal-option-1 on 120 transitions
Took 0.002869129180908203s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 554 Step: 41280
================================================================================
[Random] Deep skill graphs target event: SE([10 15])
[Random] DSG selected event SE([10 15])
[Graph Consolidation] From (4, 18) to [10 15]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 15])
Planner goal: SE([ 4 18]), DSC goal: SE([10 15]) and Goal: SE([10 15])
False
False
Rolling out DSC with goal vertex SE([10 15])
Rolling out goal-option-2, from [ 4 18] targeting {'count': 172, 'bonus': 0.07624928516630233, 'player_pos': (10, 15), 'player_x': 10, 'player_y': 15, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15792, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
7.884599142397444
Updated goal-option-2 on 120 transitions
Took 0.002987384796142578s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 555 Step: 41400
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
1.1099080905159904
Updated goal-option-1 on 120 transitions
Took 0.0029883384704589844s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 556 Step: 41520
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 798, 'bonus': 0.03539961627023943, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 22188, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.8181469575708018
Updated goal-option-1 on 120 transitions
Took 0.0031957626342773438s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 557 Step: 41640
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 0, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9177170082811191
Updated goal-option-1 on 120 transitions
Took 0.0032188892364501953s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 558 Step: 41760
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9474409431295238
Updated goal-option-1 on 120 transitions
Took 0.003229379653930664s to update distance table with 121 states and events {SE([ 4 18]), None}
================================================================================
[Consolidation] Episode: 559 Step: 41880
================================================================================
Unconnected Events: [SE([10 15]), SE([10 16])] | Probs: [[0.32471638 0.67528362]]
[BoltzmannClosest] DSG selected event: SE([10 16])
[Graph Consolidation] From (4, 18) to [10 16]
Revised planner goal vertex to SE([ 4 18]) and dsc goal vertex to SE([10 16])
Planner goal: SE([ 4 18]), DSC goal: SE([10 16]) and Goal: SE([10 16])
False
False
Rolling out DSC with goal vertex SE([10 16])
Rolling out goal-option-1, from [ 4 18] targeting {'count': 784, 'bonus': 0.03571428571428571, 'player_pos': (10, 16), 'player_x': 10, 'player_y': 16, 'room_number': 1, 'jumping': 0, 'dead': False, 'falling': False, 'uncontrollable': False, 'truncated': False, 'terminated': False, 'needs_reset': False, 'buggy_state': 0, 'inventory': '00000000', 'timestep': 15786, 'has_key': False, 'door_open': True, 'left_door_open': True, 'right_door_open': True}
[-trajReplay] Replaying negative trajectory with positive relabel
0.9486413934648344
Updated goal-option-1 on 120 transitions
Took 0.0032012462615966797s to update distance table with 121 states and events {SE([ 4 18]), None}
[Episode=560 Seed=18] Took 0.058380842208862305s to save gc logs
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 560 Step: 42000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -52.134735006373376
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 561 Step: 42000
================================================================================
Attempting to expand SE([ 4 18])
[RND Rollout] Reward: 0.0	IntrinsicReward: -56.33585074818984
<hrl.agent.dsg.dsg.SkillGraphAgent object at 0x147404723b80>
================================================================================
[Expansion] Episode: 562 Step: 42000
================================================================================
Attempting to expand SE([ 4 18])
